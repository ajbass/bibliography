%% 2001

@Article{Storey_2001,
  doi = {10.1089/106652701753216530},
  url = {https://dx.doi.org/10.1089/106652701753216530},
  year = {2001},
  month = {oct},
  publisher = {Mary Ann Liebert Inc},
  volume = {8},
  number = {5},
  pages = {549--556},
  author = {John D. Storey and David Siegmund},
  title = {Approximate p-values for local sequence alignments: Numerical studies},
  journal = {Journal of Computational Biology},
  abstract = {Siegmund and Yakir (2000) have given an approximate p-value when two independent, identically distributed sequences from a finite alphabet are optimally aligned based on a scoring system that rewards similarities according to a general scoring matrix and penalizes gaps (insertions and deletions). The approximation involves an infinite sequence of difficult-to-compute parameters. In this paper, it is shown by numerical studies that these reduce to essentially two numerically distinct parameters, which can be computed as one-dimensional numerical integrals. For an arbitrary scoring matrix and affine gap penalty, this modified approximation is easily evaluated. Comparison with published numerical results show that it is reasonably accurate.},
  data = {2001-10-01},
}

@Article{gilbert2001function,
  url = {http://www.fq.math.ca/Scanned/39-1/gilbert.pdf},
  title = {Function digraphs of quadratic maps modulo p},
  author = {Christie L Gilbert and Joseph D Kolesar and Clifford A Reiter and John D Storey},
  journal = {Fibonacci Quarterly},
  volume = {39},
  number = {1},
  pages = {32--49},
  year = {2001},
  publisher = {The Fibonacci Association},
  abstract = {In this paper we consider geometric representations of the iteration of quadratic polynomials modulo p. This is a discrete analogue of the classical quadratic Julia sets which have been the subject of much study.},
  data = {2001-02-01},
}

@Report{Storey2001BayesTechReport,
  author = {John D. Storey},
  title = {The false discovery rate: A Bayesian interpretation and the q-value},
  type = {Technical Report 2001-12},
  institution = {Department of Statistics, Stanford University},
  date = {2001-05-01},
  url = {http://storeylab.org/doc/2001-12.pdf},
  abstract = {With the growing abundance of large data sets, multiple comparison procedures continue to gain importance. For example, active areas such as wavelet analysis and genomics often require one to essentially test many hypotheses simultaneously. One multiple comparison procedure is the False Discovery Rate, which measures the expected propor- tion offalse positives among all significant hypotheses. In this paper we investigate some statistical properties of the False Discovery Rate. A Bayesian interpretation is made, and some asymptotic results are presented. Also, a new quantity called the q-value is introduced, which is the False Discovery Rate analogue of the p-value.},
}

@Report{Storey2001EBTechReport,
  author = {Brad Efron and Robert Tibshirani and John D. Storey and Virginia Tusher},
  title = {Empirical Bayes analysis of a microarray experiment},
  type = {Technical Report Bio-216},
  institution = {Department of Statistics, Stanford University},
  date = {2001-05-01},
  url = {http://storeylab.org/doc/BIO216.pdf},
  abstract = {Microarrays are a new technology that enables the simultaneous measurement of thousands of gene expression levels. A typical microarray experiment can produce millions of data points, raising serious problems of data reduction and simultaneous inference. We consider one such experiment in which oligonucleotide arrays were employed to assess the genetic effects of ionizing radiation on seven thousand human genes. A simple nonparametric empirical Bayes model is introduced that is used to guide the efficient reduction of the data to a single summary statistic per gene, and also to make simultaneous inferences concerning which genes were affected by the radiation. The empirical Bayes inferences are closely related to the frequentist False Discovery Rate criterion.},
}

@Report{Storey2001NewApproachFDRTechReport,
  author = {John D. Storey},
  title = {A new approach to false discovery rates and multiple hypothesis testing},
  type = {Technical Report 2001-18},
  institution = {Department of Statistics, Stanford University},
  date = {2001-06-01},
  url = {http://storeylab.org/doc/2001-18.pdf},
  abstract = {Testing multiple hypotheses involves guarding against much more complicated errors than when testing a single hypothesis. Whereas one typically controls the Type I error rate for a single hypothesis test, the Family Wise Error Rate (FWER) or the False Discovery Rate (FDR) are controlled for multiple hypothesis tests. Therefore, just as in single hypothesis testing, the acceptable error rate is fixed and the rejection region is found to control the error rate. Controlling the FWER or FDR often involves complicated sequential p-value rejection methods based on the observed data. In other words, the rejection region is estimated from the data. In this paper we propose the opposite approach --- fix the rejection region and then estimate the error rate. This new approach offers increased applicability, accuracy, and power. We apply this methodology to the FDR and provide evidence for its benefits. Also discussed is the calculation of the q-value, which is the FDR analogue of the p-value. Some simple numerical examples are presented that show this new approach can yield over a 10 times increase in power compared to the Benjamini and Hochberg (1995) method. We also briefly discuss how this approach can be applied to other multiple hypothesis testing error measures, such as the FWER.},
}

@Report{Efron2001TechReport,
  author = {Brad Efron and John D. Storey and Robert Tibshirani},
  title = {Microarrays, empirical Bayes methods, and false discovery rates},
  type = {Technical Report Bio-217},
  institution = {Department of Statistics, Stanford University},
  date = {2001-09-01},
  url = {http://storeylab.org/doc/BIO217.pdf},
  abstract = {In a classic two-sample problem one might use Wilcoxon's statistic to test for a difference between Treatment and Control subjects. The analogous microarray experiment yields thou- sands of Wilcoxon statistics, one for each gene on the array, and confronts the statistician with a difficult simultaneous inference situation . We will discuss two inferential approaches to this problem: an empirical Bayes method that requires very little a priori Bayesian modeling, and the frequentist method of 'False Discovery Rates' proposed by Benjamini and Hochberg in 1995. It turns out that the two methods are closely related and can be used together to produce sensible simultaneous inferences.},
}

@Report{Storey2001DepTechReport,
  author = {John D. Storey and Robert Tibshirani},
  title = {Estimating the positive false discovery rate under dependence, with applications to DNA microarrays},
  type = {Technical Report 2001-28},
  institution = {Department of Statistics, Stanford University},
  date = {2001-10-01},
  url = {http://storeylab.org/doc/2001-28.pdf},
  abstract = {When conducting multiple hypothesis tests, it is important to assess the number of false positives in some fashion. One useful error measure is the positive False Discovery Rate (pFDR). We show how to estimate the pFDR when general dependence between the hypotheses exists. This can be done using general statistics, not necessarily p-values, where the Type I error rate for a given rejection region may not even be known. We apply the proposed methodology to the problem of detecting differential gene expression in replicated DNA microarray experiments, where unknown dependence is likely to occur.},
}

@Article{Efron_2001,
  doi = {10.1198/016214501753382129},
  url = {https://dx.doi.org/10.1198/016214501753382129},
  year = {2001},
  month = {dec},
  publisher = {Informa {UK} Limited},
  volume = {96},
  number = {456},
  pages = {1151--1160},
  author = {Bradley Efron and Robert Tibshirani and John D. Storey and Virginia Tusher},
  title = {Empirical Bayes analysis of a microarray experiment},
  journal = {Journal of the American Statistical Association},
  abstract = {Microarrays are a novel technology that facilitates the simultaneous measurement of thousands of gene expression levels. A typical microarray experiment can produce millions of data points, raising serious problems of data reduction, and simultaneous inference. We consider one such experiment in which oligonucleotide arrays were employed to assess the genetic effects of ionizing radiation on seven thousand human genes. A simple nonparametric empirical Bayes model is introduced, which is used to guide the efficient reduction of the data to a single summary statistic per gene, and also to make simultaneous inferences concerning which genes were affected by the radiation. Although our focus is on one specific experiment, the proposed methods can be applied quite generally. The empirical Bayes inferences are closely related to the frequentist false discovery rate (FDR) criterion.},
  data = {2001-12-31},
}

%% 2002

@Article{Storey_2002,
  doi = {10.1111/1467-9868.00346},
  url = {https://dx.doi.org/10.1111/1467-9868.00346},
  year = {2002},
  month = {aug},
  publisher = {Wiley-Blackwell},
  volume = {64},
  number = {3},
  pages = {479--498},
  author = {John D. Storey},
  title = {A direct approach to false discovery rates},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  abstract = {Multiple-hypothesis testing involves guarding against much more complicated errors than single-hypothesis testing. Whereas we typically control the type I error rate for a single-hypothesis test, a compound error rate is controlled for multiple-hypothesis tests. For example, controlling the false discovery rate FDR traditionally involves intricate sequential p-value rejection methods based on the observed data. Whereas a sequential p-value method fixes the error rate and estimates its corresponding rejection region, we propose the opposite approach --- we fix the rejection region and then estimate its corresponding error rate. This new approach offers increased applicability, accuracy and power. We apply the methodology to both the positive false discovery rate pFDR and FDR, and provide evidence for its benefits. It is shown that pFDR is probably the quantity of interest over FDR. Also discussed is the calculation of the q-value, the pFDR analogue of the p-value, which eliminates the need to set the error rate beforehand as is traditionally done. Some simple numerical examples are presented that show that this new approach can yield an increase of over eight times in power compared with the Benjamini–Hochberg FDR method.},
  data = {2002-08-12},
}

@Article{Clement_2002,
  doi = {10.1101/gr.207702},
  url = {https://dx.doi.org/10.1101/gr.207702},
  year = {2002},
  month = {feb},
  publisher = {Cold Spring Harbor Laboratory Press},
  volume = {12},
  number = {2},
  pages = {281--291},
  author = {K. Clement},
  title = {In vivo regulation of human skeletal muscle gene expression by thyroid hormone},
  journal = {Genome Research},
  abstract = {Thyroid hormones are key regulators of metabolism that modulate transcription via nuclear receptors. Hyperthyroidism is associated with increased metabolic rate, protein breakdown, and weight loss. Although the molecular actions of thyroid hormones have been studied thoroughly, their pleiotropic effects are mediated by complex changes in expression of an unknown number of target genes. Here, we measured patterns of skeletal muscle gene expression in five healthy men treated for 14 days with 75 μg of triiodothyronine, using 24,000 cDNA element microarrays. To analyze the data, we used a new statistical method that identifies significant changes in expression and estimates the false discovery rate. The 381 up-regulated genes were involved in a wide range of cellular functions including transcriptional control, mRNA maturation, protein turnover, signal transduction, cellular trafficking, and energy metabolism. Only two genes were down-regulated. Most of the genes are novel targets of thyroid hormone. Cluster analysis of triiodothyronine-regulated gene expression among 19 different human tissues or cell lines revealed sets of coregulated genes that serve similar biologic functions. These results define molecular signatures that help to understand the physiology and pathophysiology of thyroid hormone action.},
  data = {2002-02-01},
}

@Thesis{storey2002false,
  title = {False discovery rates: Theory and applications to DNA microarrays},
  author = {John D. Storey},
  year = {2002},
  institution = {Stanford University},
  type = {PhD Dissertation},
  url = {https://searchworks.stanford.edu/view/5417184},
  abstract = {Multiple hypothesis testing is concerned with appropriately controlling the rate of false positives when testing several hypotheses simultaneously, while maintaining the power of each test as much as possible. One multiple hypothesis testing error measure is the False Discovery Rate (FDR), which is loosely defined to be the expected proportion of false pos- itives among all significant hypotheses. The FDR is especially appropriate for exploratory analyses in which one is interested in finding many significant results among many tests. In this work, we introduce a modified version of the FDR called the 'positive False Discovery Rate' (pFDR). We argue the pFDR is a more appropriate and useful error measure, and we investigate its statistical properties. When assuming the test statistics come from a mixture distribution, we show the pFDR can be written as a posterior probability and can be connected to classification theory. These properties remain asymptotically true under fairly general conditions, even under certain forms of dependence. Also, a new quantity called the 'q-value' is introduced and investigated, which is a natural 'Bayesian p-value', or rather the pFDR analogue of the p-value. This idea is also generalized to any multiple hypothesis testing error measure. Using these results, we introduce point estimates of the FDR and pFDR for fixed rejection regions. The point estimates provide proper conservative behavior in the three scenarios of (1) estimating false discovery rates for fixed rejection regions, (2) estimating rejection regions for fixed false discovery rates, and (3) simultaneously estimating false discovery rates over all possible rejection regions –-- even under certain forms of dependence. It is shown that this new set of methodology extends the current methodology and also provides increases in power. We apply the methodology to the problem of de- tecting differential gene expression between two or more biological samples based on DNA microarray data. This application is well suited because the dependence between the tests (genes) is weak and the number of tests is quite large.},
  data = {2002-06-01},
}

@Article{Wang_2002,
  doi = {10.1073/pnas.092538799},
  url = {https://dx.doi.org/10.1073/pnas.092538799},
  year = {2002},
  month = {apr},
  publisher = {Proceedings of the National Academy of Sciences},
  volume = {99},
  number = {9},
  pages = {5860--5865},
  author = {Y. Wang and C. L. Liu and John D. Storey and R. J. Tibshirani and D. Herschlag and P. O. Brown},
  title = {Precision and functional specificity in {mRNA} decay},
  journal = {Proceedings of the National Academy of Sciences},
  abstract = {Posttranscriptional processing of mRNA is an integral component of the gene expression program. By using DNA microarrays, we precisely measured the decay of each yeast mRNA, after thermal inactivation of a temperature-sensitive RNA polymerase II. The half-lives varied widely, ranging from ∼3 min to more than 90 min. We found no simple correlation between mRNA half-lives and ORF size, codon bias, ribosome density, or abundance. However, the decay rates of mRNAs encoding groups of proteins that act together in stoichiometric complexes were generally closely matched, and other evidence pointed to a more general relationship between physiological function and mRNA turnover rates. The results provide strong evidence that precise control of the decay of each mRNA is a fundamental feature of the gene expression program in yeast.},
  data = {2002-04-30},
}

%% 2003

@Article{Arava_2003,
  doi = {10.1073/pnas.0635171100},
  url = {https://dx.doi.org/10.1073/pnas.0635171100},
  year = {2003},
  month = {mar},
  publisher = {Proceedings of the National Academy of Sciences},
  volume = {100},
  number = {7},
  pages = {3889--3894},
  author = {Y. Arava and Y. Wang and John D. Storey and C. L. Liu and P. O. Brown and D. Herschlag},
  title = {Genome-wide analysis of {mRNA} translation profiles in Saccharomyces cerevisiae},
  journal = {Proceedings of the National Academy of Sciences},
  abstract = {We have analyzed the translational status of each mRNA in rapidly growing Saccharomyces cerevisiae. mRNAs were separated by velocity sedimentation on a sucrose gradient, and 14 fractions across the gradient were analyzed by quantitative microarray analysis, providing a profile of ribosome association with mRNAs for thousands of genes. For most genes, the majority of mRNA molecules were associated with ribosomes and presumably engaged in translation. This systematic approach enabled us to recognize genes with unusual behavior. For 43 genes, most mRNA molecules were not associated with ribosomes, suggesting that they may be translationally controlled. For 53 genes, including GCN4, CPA1, and ICY2, three genes for which translational control is known to play a key role in regulation, most mRNA molecules were associated with a single ribosome. The number of ribosomes associated with mRNAs increased with increasing length of the putative protein-coding sequence, consistent with longer transit times for ribosomes translating longer coding sequences. The density at which ribosomes were distributed on each mRNA (i.e., the number of ribosomes per unit ORF length) was well below the maximum packing density for nearly all mRNAs, consistent with initiation as the rate-limiting step in translation. Global analysis revealed an unexpected correlation: Ribosome density decreases with increasing ORF length. Models to account for this surprising observation are discussed.},
  data = {2003-04-01},
}

@Article{Storey_2003,
  doi = {10.1214/aos/1074290335},
  url = {https://dx.doi.org/10.1214/aos/1074290335},
  year = {2003},
  month = {dec},
  publisher = {Institute of Mathematical Statistics},
  volume = {31},
  number = {6},
  pages = {2013--2035},
  author = {John D. Storey},
  title = {The positive false discovery rate: A Bayesian interpretation and the q-value},
  journal = {Annals of Statistics},
  abstract = {Multiple hypothesis testing is concerned with controlling the rate of false positives when testing several hypotheses simultaneously. One multiple hypothesis testing error measure is the false discovery rate (FDR), which is loosely defined to be the expected proportion of false positives among all significant hypotheses. The FDR is especially appropriate for exploratory analyses in which one is interested in finding several significant results among many tests. In this work, we introduce a modified version of the FDR called the 'positive false discovery rate' (pFDR). We discuss the advantages and disadvantages of the pFDR and investigate its statistical properties. When assuming the test statistics follow a mixture distribution, we show that the pFDR can be written as a Bayesian posterior probability and can be connected to classification theory. These properties remain asymptotically true under fairly general conditions, even under certain forms of dependence. Also, a new quantity called the 'q-value' is introduced and investigated, which is a natural Bayesian posterior p-value, or rather the pFDR analogue of the p-value.},
  data = {2003-12-01},
}

@InCollection{storey2003sam,
  doi = {10.1007/b97411},
  url = {https://dx.doi.org/10.1007/b97411},
  title = {SAM thresholding and false discovery rates for detecting differential gene expression in DNA microarrays},
  author = {John D Storey and Robert Tibshirani},
  booktitle = {The Analysis of Gene Expression Data: Methods and Software},
  editor = {Giovanni Parmigiani and Elizabeth S. Garett and Rafael A. Irizarry and Scott L Zeger},
  pages = {272--290},
  year = {2003},
  publisher = {Springer New York},
  abstract = {SAM is a computer package for correlating gene expression with an outcome parameter such as treatment, survival time, or diagnostic class. It thresholds an appropriate test statistic and reports the q-value of each test based on a set of sample permutations. SAM works as a Microsoft Excel add-in and has additional features for fold-change thresholding and block permutations. Here, we explain how the SAM methodology works in the context of a general approach to detecting differential gene expression in DNA microarrays. Some recently developed methodology for estimating false discovery rates and q-values has been included in the SAM software, which we summarize here.},
  data = {2003-04-08},
}

@InCollection{storey2003statistical,
  doi = {10.1385/1-59259-364-x:149},
  url = {https://dx.doi.org/10.1385/1-59259-364-X:149},
  year = {2003},
  publisher = {Springer Nature},
  pages = {149--158},
  author = {John D. Storey and Robert Tibshirani},
  title = {Statistical methods for identifying differentially expressed genes in {DNA} microarrays},
  editor = {Michael Kaufmann and Claudia Klinger},
  booktitle = {Functional Genomics: Methods and Protocols},
  abstract = {In this chapter we discuss the problem of identifying differentially expressed genes from a set of microarray experiments. Statistically speaking, this task falls under the heading of 'multiple hypothesis testing.' In other words, we must perform hypothesis tests on all genes simultaneously to determine whether each one is differentially expressed. Recall that in statistical hypothesis testing, we test a null hypothesis vs an alternative hypothesis. In this example, the null hypothesis is that there is no change in expression levels between experimental conditions. The alternative hypothesis is that there is some change. We reject the null hypothesis if there is enough evidence in favor of the alternative. This amounts to rejecting the null hypothesis if its corresponding statistic falls into some predetermined rejection region. Hypothesis testing is also concerned with measuring the probability of rejecting the null hypothesis when it is really true (called a false positive), and the probability of rejecting the null hypothesis when the alternative hypothesis is really true (called power).},
  data = {2003-03-13},
}

@Article{Storey_2003_PNAS,
  doi = {10.1073/pnas.1530509100},
  url = {https://dx.doi.org/10.1073/pnas.1530509100},
  year = {2003},
  month = {jul},
  publisher = {Proceedings of the National Academy of Sciences},
  volume = {100},
  number = {16},
  pages = {9440--9445},
  author = {John D. Storey and R. Tibshirani},
  title = {Statistical significance for genomewide studies},
  journal = {Proceedings of the National Academy of Sciences},
  abstract = {With the increase in genomewide experiments and the sequencing of multiple genomes, the analysis of large data sets has become commonplace in biology. It is often the case that thousands of features in a genomewide data set are tested against some null hypothesis, where a number of features are expected to be significant. Here we propose an approach to measuring statistical significance in these genomewide studies based on the concept of the false discovery rate. This approach offers a sensible balance between the number of true and false positives that is automatically calibrated and easily interpreted. In doing so, a measure of statistical significance called the q-value is associated with each tested feature. The q-value is similar to the well known p-value, except it is a measure of significance in terms of the false discovery rate rather than the false positive rate. Our approach avoids a flood of false positive results, while offering a more liberal criterion than what has been used in genome scans for linkage.},
  date = {2003-08-05},
}

%% 2004

@Article{Vaszar_2004,
  doi = {10.1152/physiolgenomics.00198.2003},
  url = {https://dx.doi.org/10.1152/physiolgenomics.00198.2003},
  year = {2004},
  month = {apr},
  publisher = {American Physiological Society},
  volume = {17},
  number = {2},
  pages = {150--156},
  author = {Laszlo T. Vaszar and Toshihiko Nishimura and John D. Storey and Guohua Zhao and Daoming Qiu and John L. Faul and Ronald G. Pearl and Peter N. Kao},
  title = {Longitudinal transcriptional analysis of developing neointimal vascular occlusion and pulmonary hypertension in rats},
  journal = {Physiological Genomics},
  abstract = {Pneumonectomized rats injected with the alkaloid toxin, monocrotaline, develop progressive neointimal pulmonary vascular obliteration and pulmonary hypertension resulting in right ventricular failure and death. The antiproliferative immunosuppressant, triptolide, attenuates neointimal formation and pulmonary hypertension in this disease model. Pneumonectomized rats, injected with monocrotaline on day 7, were killed at days 14, 21, 28, and 35 for measurements of physiology and gene expression patterns. These data were compared with pneumonectomized, monocrotaline-injected animals that received triptolide from day 5 to day 35. The hypothesis was tested that a group of functionally related genes would be significantly coexpressed during the development of disease and downregulated in response to treatment. Transcriptional analysis using total lung RNA was performed on replicate animals for each experimental time point with exploratory data analysis followed by statistical significance analysis. Marked, statistically significant increases in proteases (particularly derived from mast cells) were noted that parallel the development of vascular obliteration and pulmonary hypertension. Mast-cell-derived proteases may play a role in regulating the development of neointimal pulmonary vascular occlusion and pulmonary hypertension in response to injury.},
  date = {2004-04-01},
}

@Article{Storey_2004,
  doi = {10.1111/j.1467-9868.2004.00439.x},
  url = {https://dx.doi.org/10.1111/j.1467-9868.2004.00439.x},
  year = {2004},
  month = {feb},
  publisher = {Wiley-Blackwell},
  volume = {66},
  number = {1},
  pages = {187--205},
  author = {John D. Storey and Jonathan E. Taylor and David Siegmund},
  title = {Strong control, conservative point estimation and simultaneous conservative consistency of false discovery rates: A unified approach},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  abstract = {The false discovery rate (FDR) is a multiple hypothesis testing quantity that describes the expected proportion of false positive results among all rejected null hypotheses. Benjamini and Hochberg introduced this quantity and proved that a particular step-up p-value method controls the FDR. Storey introduced a point estimate of the FDR for fixed significance regions. The former approach conservatively controls the FDR at a fixed predetermined level, and the latter provides a conservatively biased estimate of the FDR for a fixed predetermined significance region. In this work, we show in both finite sample and asymptotic settings that the goals of the two approaches are essentially equivalent. In particular, the FDR point estimates can be used to define valid FDR controlling procedures. In the asymptotic setting, we also show that the point estimates can be used to estimate the FDR conservatively over all significance regions simultaneously, which is equivalent to controlling the FDR at all levels simultaneously. The main tool that we use is to translate existing FDR methods into procedures involving empirical processes. This simplifies finite sample proofs, provides a framework for asymptotic results and proves that these procedures are valid even under certain forms of dependence.},
  date = {2004-02-01},
}

%% 2005

@Article{Brem_2005,
  doi = {10.1038/nature03865},
  url = {https://dx.doi.org/10.1038/nature03865},
  year = {2005},
  month = {aug},
  publisher = {Springer Nature},
  volume = {436},
  number = {7051},
  pages = {701--703},
  author = {Rachel B. Brem and John D. Storey and Jacqueline Whittle and Leonid Kruglyak},
  title = {Genetic interactions between polymorphisms that affect gene expression in yeast},
  journal = {Nature},
  abstract = {Interactions between polymorphisms at different quantitative trait loci (QTLs) are thought to contribute to the genetics of many traits, and can markedly affect the power of genetic studies to detect QTLs. Interacting loci have been identified in many organisms. However, the prevalence of interactions, and the nucleotide changes underlying them, are largely unknown. Here we search for naturally occurring genetic interactions in a large set of quantitative phenotypes --- the levels of all transcripts in a cross between two strains of Saccharomyces cerevisiae. For each transcript, we searched for secondary loci interacting with primary QTLs detected by their individual effects. Such locus pairs were estimated to be involved in the inheritance of 57% of transcripts; statistically significant pairs were identified for 225 transcripts. Among these, 67% of secondary loci had individual effects too small to be significant in a genome-wide scan. Engineered polymorphisms in isogenic strains confirmed an interaction between the mating-type locus MAT and the pheromone response gene GPA1. Our results indicate that genetic interactions are widespread in the genetics of transcript levels, and that many QTLs will be missed by single-locus tests but can be detected by two-stage tests that allow for interactions.},
  date = {2005-08-04},
}

@Article{Storey_2005,
  doi = {10.1371/journal.pbio.0030267},
  url = {https://dx.doi.org/10.1371/journal.pbio.0030267},
  year = {2005},
  month = {jul},
  publisher = {Public Library of Science ({PLoS})},
  volume = {3},
  number = {8},
  pages = {e267},
  author = {John D Storey and Joshua M Akey and Leonid Kruglyak},
  title = {Multiple locus linkage analysis of genomewide expression in yeast},
  journal = {{PLoS} Biology},
  abstract = {With the ability to measure thousands of related phenotypes from a single biological sample, it is now feasible to genetically dissect systems-level biological phenomena. The genetics of transcriptional regulation and protein abundance are likely to be complex, meaning that genetic variation at multiple loci will influence these phenotypes. Several recent studies have investigated the role of genetic variation in transcription by applying traditional linkage analysis methods to genomewide expression data, where each gene expression level was treated as a quantitative trait and analyzed separately from one another. Here, we develop a new, computationally efficient method for simultaneously mapping multiple gene expression quantitative trait loci that directly uses all of the available data. Information shared across gene expression traits is captured in a way that makes minimal assumptions about the statistical properties of the data. The method produces easy-to-interpret measures of statistical significance for both individual loci and the overall joint significance of multiple loci selected for a given expression trait. We apply the new method to a cross between two strains of the budding yeast Saccharomyces cerevisiae, and estimate that at least 37% of all gene expression traits show two simultaneous linkages, where we have allowed for epistatic interactions. Pairs of jointly linking quantitative trait loci are identified with high confidence for 170 gene expression traits, where it is expected that both loci are true positives for at least 153 traits. In addition, we are able to show that epistatic interactions contribute to gene expression variation for at least 14% of all traits. We compare the proposed approach to an exhaustive two-dimensional scan over all pairs of loci. Surprisingly, we demonstrate that an exhaustive two-dimensional scan is less powerful than the sequential search used here. In addition, we show that a two-dimensional scan does not truly allow one to test for simultaneous linkage, and the statistical significance measured from this existing method cannot be interpreted among many traits.},
  date = {2005-07-26},
}

@Article{Storey_2005:1,
  doi = {10.1073/pnas.0504609102},
  url = {https://dx.doi.org/10.1073/pnas.0504609102},
  year = {2005},
  month = {sep},
  publisher = {Proceedings of the National Academy of Sciences},
  volume = {102},
  number = {36},
  pages = {12837--12842},
  author = {John D. Storey and W. Xiao and J. T. Leek and R. G. Tompkins and R. W. Davis},
  title = {Significance analysis of time course microarray experiments},
  journal = {Proceedings of the National Academy of Sciences},
  abstract = {Characterizing the genome-wide dynamic regulation of gene expression is important and will be of much interest in the future. However, there is currently no established method for identifying differentially expressed genes in a time course study. Here we propose a significance method for analyzing time course microarray studies that can be applied to the typical types of comparisons and sampling schemes. This method is applied to two studies on humans. In one study, genes are identified that show differential expression over time in response to in vivo endotoxin administration. By using our method, 7,409 genes are called significant at a 1% false discovery rate level, whereas several existing approaches fail to identify any genes. In another study, 417 genes are identified at a 10% false discovery rate level that show expression changing with age in the kidney cortex. Here it is also shown that as many as 47% of the genes change with age in a manner more complex than simple exponential growth or decay. The methodology proposed here has been implemented in the freely distributed and open-source EDGE software package.},
  date = {2005-09-06},
}

@Article{Leek_2005,
  doi = {10.1093/bioinformatics/btk005},
  url = {https://dx.doi.org/10.1093/bioinformatics/btk005},
  year = {2005},
  month = {dec},
  publisher = {Oxford University Press ({OUP})},
  volume = {22},
  number = {4},
  pages = {507--508},
  author = {J. T. Leek and E. Monsen and A. R. Dabney and John D. Storey},
  title = {Edge: extraction and analysis of differential gene expression},
  journal = {Bioinformatics},
  abstract = {EDGE (Extraction of Differential Gene Expression) is an open source, point-and-click software program for the significance analysis of DNA microarray experiments. EDGE can perform both standard and time course differential expression analysis. The functions are based on newly developed statistical theory and methods. This document introduces the EDGE software package.},
  date = {2005-02-15},
}

%% 2006

@Article{Chen_2006,
  doi = {10.1534/genetics.105.052506},
  url = {https://dx.doi.org/10.1534/genetics.105.052506},
  year = {2006},
  month = {may},
  publisher = {Genetics Society of America},
  volume = {173},
  number = {4},
  pages = {2371--2381},
  author = {L. Chen},
  title = {Relaxed significance criteria for linkage analysis},
  journal = {Genetics},
  abstract = {Linkage analysis involves performing significance tests at many loci located throughout the genome. Traditional criteria for declaring a linkage statistically significant have been formulated with the goal of controlling the rate at which any single false positive occurs, called the genomewise error rate (GWER). As complex traits have become the focus of linkage analysis, it is increasingly common to expect that a number of loci are truly linked to the trait. This is especially true in mapping quantitative trait loci (QTL), where sometimes dozens of QTL may exist. Therefore, alternatives to the strict goal of preventing any single false positive have recently been explored, such as the false discovery rate (FDR) criterion. Here, we characterize some of the challenges that arise when defining relaxed significance criteria that allow for at least one false positive linkage to occur. In particular, we show that the FDR suffers from several problems when applied to linkage analysis of a single trait. We therefore conclude that the general applicability of FDR for declaring significant linkages in the analysis of a single trait is dubious. Instead, we propose a significance criterion that is more relaxed than the traditional GWER, but does not appear to suffer from the problems of the FDR. A generalized version of the GWER is proposed, called GWER$_k$, that allows one to provide a more liberal balance between true positives and false positives at no additional cost in computation or assumptions.},
  date = {2006-08-01},
}

@Article{Dabney2006,
  doi = {10.1186/gb-2006-7-3-401},
  url = {https://dx.doi.org/10.1186/gb-2006-7-3-401},
  year  = {2006},
  publisher = {Springer Nature},
  volume = {7},
  number = {3},
  pages = {401},
  author = {Alan R Dabney and John D Storey},
  title = {A reanalysis of a published Affymetrix GeneChip control dataset},
  journal = {Genome Biology}
  abstract = {In a recent Genome Biology article, Choe et al. described a control dataset for Affymetrix GeneChips. By spiking RNA at known quantities, the identities of all null and differentially expressed genes are known exactly, as well as the fold change of differential expression. With the wealth of analysis methods available for microarray data, a control dataset would be very useful. Unfortunately, serious errors are evident in the Choe et al. data, disproving their conclusions and implying that the dataset cannot be used to validly evaluate statistical inference methods. We argue that problems in the dataset are at least partially due to a flaw in the experimental design.},
  date = {2006-03-22},
}

@Article{Dabney_2006:1,
  doi = {10.1093/biostatistics/kxj038},
  url = {https://dx.doi.org/10.1093/biostatistics/kxj038},
  year = {2006},
  month = {apr},
  publisher = {Oxford University Press ({OUP})},
  volume = {8},
  number = {1},
  pages = {128--139},
  author = {A. R. Dabney and John D. Storey},
  title = {A new approach to intensity-dependent normalization of two-channel microarrays},
  journal = {Biostatistics},
  abstract = {A two-channel microarray measures the relative expression levels of thousands of genes from a pair of biological samples. In order to reliably compare gene expression levels between and within arrays, it is necessary to remove systematic errors that distort the biological signal of interest. The standard for accomplishing this is smoothing 'MA-plots' to remove intensity-dependent dye bias and array-specific effects. However, MA methods require strong assumptions, which limit their general applicability. We review these assumptions and derive several practical scenarios in which they fail. The 'dye-swap' normalization method has been much less frequently used because it requires two arrays per pair of samples. We show that a dye-swap is accurate under general assumptions, even under intensity-dependent dye bias, and that a dye-swap removes dye bias from a single pair of samples in general. Based on a flexible model of the relationship between mRNA amount and single-channel fluorescence intensity, we demonstrate the general applicability of a dye-swap approach. We then propose a common array dye-swap (CADS) method for the normalization of two-channel microarrays. We show that CADS removes both dye bias and array-specific effects, and preserves the true differential expression signal for every gene under the assumptions of the model.},
  date = {2006-04-24},
}

@Article{Storey_2006,
  doi = {10.1093/biostatistics/kxl019},
  url = {https://dx.doi.org/10.1093/biostatistics/kxl019},
  year = {2006},
  month = {aug},
  publisher = {Oxford University Press ({OUP})},
  volume = {8},
  number = {2},
  pages = {414--432},
  author = {John D. Storey and J. Y. Dai and J. T. Leek},
  title = {The optimal discovery procedure for large-scale significance testing, with applications to comparative microarray experiments},
  journal = {Biostatistics},
  abstract = {As much of the focus of genetics and molecular biology has shifted toward the systems level, it has become increasingly important to accurately extract biologically relevant signal from thousands of related measurements. The common property among these high-dimensional biological studies is that the measured features have a rich and largely unknown underlying structure. One example of much recent interest is identifying differentially expressed genes in comparative microarray experiments. We propose a new approach aimed at optimally performing many hypothesis tests in a high-dimensional study. This approach estimates the optimal discovery procedure (ODP), which has recently been introduced and theoretically shown to optimally perform multiple significance tests. Whereas existing procedures essentially use data from only one feature at a time, the ODP approach uses the relevant information from the entire data set when testing each feature. In particular, we propose a generally applicable estimate of the ODP for identifying differentially expressed genes in microarray experiments. This microarray method consistently shows favorable performance over five highly used existing methods. For example, in testing for differential expression between two breast cancer tumor types, the ODP provides increases from 72% to 185% in the number of genes called significant at a false discovery rate of 3%. Our proposed microarray method is freely available to academic users in the open-source, point-and-click EDGE software package.},
  date = {2006-08-23},
}

%% 2007

@Article{Akey_2007,
  doi = {10.1038/ng0707-807},
  url = {https://dx.doi.org/10.1038/ng0707-807},
  year = {2007},
  month = {jul},
  publisher = {Springer Nature},
  volume = {39},
  number = {7},
  pages = {807--808},
  author = {Joshua M Akey and Shameek Biswas and Jeffrey T Leek and John D Storey},
  title = {On the design and analysis of gene expression studies in human populations},
  journal = {Nature Genetics},
  abstract = {In a recent Nature Genetics Letter entitled 'Common genetic variants account for differences in gene expression among ethnic groups', Spielman et al. estimate the number of genes differentially expressed between individuals of European (CEU) and Asian (ASN) ancestry and suggest that these differences can be accounted for by measured genetic variants. We recently performed a similar study comparing differences in gene expression among individuals of European and Yoruban ancestry. Given the scientific, medical and societal implications of this research area, it is important for the scientific community to carefully revisit and critically evaluate the conclusions of such studies. To this end, we have reanalyzed the data in Spielman et al. to provide a common basis for comparison with our study. In doing so, we found that important issues arise about the accuracy of their results.},
  date = {2007-07-01},
}

@Article{Chen_2007,
  doi = {10.1186/gb-2007-8-10-r219},
  url = {https://dx.doi.org/10.1186/gb-2007-8-10-r219},
  year = {2007},
  publisher = {Springer Nature},
  volume = {8},
  number = {10},
  pages = {R219},
  author = {Lin S Chen and Frank Emmert-Streib and John D Storey},
  title = {Harnessing naturally randomized transcription to infer regulatory relationships among genes},
  journal = {Genome Biology},
  abstract = {We develop an approach utilizing randomized genotypes to rigorously infer causal regulatory relationships among genes at the transcriptional level, based on experiments in which genotyping and expression profiling are performed. This approach can be used to build transcriptional regulatory networks and to identify putative regulators of genes. We apply the method to an experiment in yeast, in which genes known to be in the same processes and functions are recovered in the resulting transcriptional regulatory network.},
  date = {2007-10-11},
}

@Article{Dabney_2007_eCADS,
  doi = {10.1186/gb-2007-8-3-r44},
  url = {https://dx.doi.org/10.1186/gb-2007-8-3-r44},
  year = {2007},
  publisher = {Springer Nature},
  volume = {8},
  number = {3},
  pages = {R44},
  author = {Alan R Dabney and John D Storey},
  title = {Normalization of two-channel microarrays accounting for experimental design and intensity-dependent relationships},
  journal = {Genome Biology},
  abstract = {In normalizing two-channel expression arrays, the ANOVA approach explicitly incorporates the experimental design in its model, and the MA plot-based approach accounts for intensity-dependent biases. However, both approaches can lead to inaccurate normalization in fairly common scenarios. We propose a method called efficient Common Array Dye Swap (eCADS) for normalizing two-channel microarrays that accounts for both experimental design and intensity-dependent biases. Under reasonable experimental designs, eCADS preserves differential expression relationships and requires only a single array per sample pair.},
  date = {2007-03-28},
}

@Article{Dabney_2007_clanc,
  doi = {10.1371/journal.pone.0001002},
  url = {https://dx.doi.org/10.1371/journal.pone.0001002},
  year = {2007},
  month = {oct},
  publisher = {Public Library of Science ({PLoS})},
  volume = {2},
  number = {10},
  pages = {e1002},
  author = {Alan R. Dabney and John D. Storey},
  editor = {Ji Zhu},
  title = {Optimality driven nearest centroid classification from genomic data},
  journal = {{PLoS} {ONE}},
  abstract = {Nearest-centroid classifiers have recently been successfully employed in high-dimensional applications, such as in genomics. A necessary step when building a classifier for high-dimensional data is feature selection. Feature selection is frequently carried out by computing univariate scores for each feature individually, without consideration for how a subset of features performs as a whole. We introduce a new feature selection approach for high-dimensional nearest centroid classifiers that instead is based on the theoretically optimal choice of a given number of features, which we determine directly here. This allows us to develop a new greedy algorithm to estimate this optimal nearest-centroid classifier with a given number of features. In addition, whereas the centroids are usually formed from maximum likelihood estimates, we investigate the applicability of high-dimensional shrinkage estimates of centroids. We apply the proposed method to clinical classification based on gene-expression microarrays, demonstrating that the proposed method can outperform existing nearest centroid classifiers.},
  date = {2007-10-03},
}

@Article{Leek_2007,
  doi = {10.1371/journal.pgen.0030161},
  url = {https://dx.doi.org/10.1371/journal.pgen.0030161},
  year = {2007},
  publisher = {Public Library of Science ({PLoS})},
  volume = {3},
  number = {9},
  pages = {e161},
  author = {Jeffrey T. Leek and John D. Storey},
  title = {Capturing heterogeneity in gene expression studies by surrogate variable analysis},
  journal = {{PLoS} Genetics},
  abstract = {It has unambiguously been shown that genetic, environmental, demographic, and technical factors may have substantial effects on gene expression levels. In addition to the measured variable(s) of interest, there will tend to be sources of signal due to factors that are unknown, unmeasured, or too complicated to capture through simple models. We show that failing to incorporate these sources of heterogeneity into an analysis can have widespread and detrimental effects on the study. Not only can this reduce power or induce unwanted dependence across genes, but it can also introduce sources of spurious signal to many genes. This phenomenon is true even for well-designed, randomized studies. We introduce 'surrogate variable analysis' (SVA) to overcome the problems caused by heterogeneity in expression studies. SVA can be applied in conjunction with standard analysis techniques to accurately capture the relationship between expression and any modeled variables of interest. We apply SVA to disease class, time course, and genetics of gene expression studies. We show that SVA increases the biological accuracy and reproducibility of analyses in genome-wide expression studies.},
  date = {2007-09-28},
}

@Article{Storey_2007_odp,
  doi = {10.1111/j.1467-9868.2007.005592.x},
  url = {https://dx.doi.org/10.1111/j.1467-9868.2007.005592.x},
  year = {2007},
  month = {jun},
  publisher = {Wiley-Blackwell},
  volume = {69},
  number = {3},
  pages = {347--368},
  author = {John D. Storey},
  title = {The optimal discovery procedure: a new approach to simultaneous significance testing},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  abstract = {The Neyman–Pearson lemma provides a simple procedure for optimally testing a single hypothesis when the null and alternative distributions are known. This result has played a major role in the development of significance testing strategies that are used in practice. Most of the work extending single-testing strategies to multiple tests has focused on formulating and estimating new types of significance measures, such as the false discovery rate. These methods tend to be based on p-values that are calculated from each test individually, ignoring information from the other tests. I show here that one can improve the overall performance of multiple significance tests by borrowing information across all the tests when assessing the relative significance of each one, rather than calculating p-values for each test individually. The 'optimal discovery procedure' is introduced, which shows how to maximize the number of expected true positive results for each fixed number of expected false positive results. The optimality that is achieved by this procedure is shown to be closely related to optimality in terms of the false discovery rate. The optimal discovery procedure motivates a new approach to testing multiple hypotheses, especially when the tests are related. As a simple example, a new simultaneous procedure for testing several normal means is defined; this is surprisingly demonstrated to outperform the optimal single-test procedure, showing that a method which is optimal for single tests may no longer be optimal for multiple tests. Connections to other concepts in statistics are discussed, including Stein's paradox, shrinkage estimation and the Bayesian approach to hypothesis testing.},
  date = {2007-06-01},
}

@Article{Storey_2007_human,
  doi = {10.1086/512017},
  url = {https://dx.doi.org/10.1086/512017},
  year = {2007},
  month = {mar},
  publisher = {Elsevier {BV}},
  volume = {80},
  number = {3},
  pages = {502--509},
  author = {John D. Storey and Jennifer Madeoy and Jeanna L. Strout and Mark Wurfel and James Ronald and Joshua M. Akey},
  title = {Gene-Expression Variation Within and Among Human Populations},
  journal = {The American Journal of Human Genetics},
  abstract = {Understanding patterns of gene-expression variation within and among human populations will provide important insights into the molecular basis of phenotypic diversity and the interpretation of patterns of expression variation in disease. However, little is known about how gene-expression variation is apportioned within and among human populations. Here, we characterize patterns of natural gene-expression variation in 16 individuals of European and African ancestry. We find extensive variation in gene-expression levels and estimate that ∼83% of genes are differentially expressed among individuals and that ∼17% of genes are differentially expressed among populations. By decomposing total gene-expression variation into within- versus among-population components, we find that most expression variation is due to variation among individuals rather than among populations, which parallels observations of extant patterns of human genetic variation. Finally, we performed allele-specific quantitative polymerase chain reaction to demonstrate that cis-regulatory variation in the lymphocyte adaptor protein (SH2B adapter protein 3) contributes to differential expression between European and African samples. These results provide the first insight into how human population structure manifests itself in gene-expression levels and will help guide the search for regulatory quantitative trait loci.},
  date = {2007-01-11},
}

%% 2008

@Article{Biswas_2008,
  doi = {10.1186/1471-2105-9-244},
  url = {https://dx.doi.org/10.1186/1471-2105-9-244},
  year = {2008},
  publisher = {Springer Nature},
  volume = {9},
  number = {1},
  pages = {244},
  author = {Shameek Biswas and John D Storey and Joshua M Akey},
  title = {Mapping gene expression quantitative trait loci by singular value decomposition and independent component analysis},
  journal = {{BMC} Bioinformatics},
  abstract = {Background: The combination of gene expression profiling with linkage analysis has become a powerful paradigm for mapping gene expression quantitative trait loci (eQTL). To date, most studies have searched for eQTL by analyzing gene expression traits one at a time. As thousands of expression traits are typically analyzed, this can reduce power because of the need to correct for the number of hypothesis tests performed. In addition, gene expression traits exhibit a complex correlation structure, which is ignored when analyzing traits individually. Results: To address these issues, we applied two different multivariate dimension reduction techniques, the Singular Value Decomposition (SVD) and Independent Component Analysis (ICA) to gene expression traits derived from a cross between two strains of Saccharomyces cerevisiae. Both methods decompose the data into a set of meta-traits, which are linear combinations of all the expression traits. The meta-traits were enriched for several Gene Ontology categories including metabolic pathways, stress response, RNA processing, ion transport, retro-transposition and telomeric maintenance. Genome-wide linkage analysis was performed on the top 20 meta-traits from both techniques. In total, 21 eQTL were found, of which 11 are novel. Interestingly, both cis and trans-linkages to the meta-traits were observed. Conclusion: These results demonstrate that dimension reduction methods are a useful and complementary approach for probing the genetic architecture of gene expression variation.},
  date = {2008-05-20},
}

@Article{Chen_2008,
  doi = {10.1093/bioinformatics/btn411},
  url = {https://dx.doi.org/10.1093/bioinformatics/btn411},
  year = {2008},
  month = {aug},
  publisher = {Oxford University Press ({OUP})},
  volume = {24},
  number = {19},
  pages = {2260--2262},
  author = {L. S. Chen and John D. Storey},
  title = {Eigen-$R^2$ for dissecting variation in high-dimensional studies},
  journal = {Bioinformatics},
  abstract = {We provide a new statistical algorithm and software package called eigen-$R^2$ for dissecting the variation of a high-dimensional biological dataset with respect to other measured variables of interest. We apply eigen-$R^2$ to two real-life examples and compare it with simply averaging $R^2$ over many features.},
  date = {2008-08-20},
}

@Article{Hao_2008,
  doi = {10.1371/journal.pgen.1000109},
  url = {https://dx.doi.org/10.1371/journal.pgen.1000109},
  year = {2008},
  month = {jun},
  publisher = {Public Library of Science ({PLoS})},
  volume = {4},
  number = {6},
  pages = {e1000109},
  author = {Ke Hao and Eric E. Schadt and John D. Storey},
  editor = {Gon{\c c}alo R. Abecasis},
  title = {Calibrating the performance of {SNP} arrays for whole-genome association studies},
  journal = {{PLoS} Genetics},
  abstract = {To facilitate whole-genome association studies (WGAS), several high-density SNP genotyping arrays have been developed. Genetic coverage and statistical power are the primary benchmark metrics in evaluating the performance of SNP arrays. Ideally, such evaluations would be done on a SNP set and a cohort of individuals that are both independently sampled from the original SNPs and individuals used in developing the arrays. Without utilization of an independent test set, previous estimates of genetic coverage and statistical power may be subject to an overfitting bias. Additionally, the SNP arrays' statistical power in WGAS has not been systematically assessed on real traits. One robust setting for doing so is to evaluate statistical power on thousands of traits measured from a single set of individuals. In this study, 359 newly sampled Americans of European descent were genotyped using both Affymetrix 500K (Affx500K) and Illumina 650Y (Ilmn650K) SNP arrays. From these data, we were able to obtain estimates of genetic coverage, which are robust to overfitting, by constructing an independent test set from among these genotypes and individuals. Furthermore, we collected liver tissue RNA from the participants and profiled these samples on a comprehensive gene expression microarray. The RNA levels were used as a large-scale set of quantitative traits to calibrate the relative statistical power of the commercial arrays. Our genetic coverage estimates are lower than previous reports, providing evidence that previous estimates may be inflated due to overfitting. The Ilmn650K platform showed reasonable power (50% or greater) to detect SNPs associated with quantitative traits when the signal-to-noise ratio (SNR) is greater than or equal to 0.5 and the causal SNP's minor allele frequency (MAF) is greater than or equal to 20% (N = 359). In testing each of the more than 40,000 gene expression traits for association to each of the SNPs on the Ilmn650K and Affx500K arrays, we found that the Ilmn650K yielded 15% times more discoveries than the Affx500K at the same false discovery rate (FDR) level.},
  date = {2008-06-27},
}

@Article{Idaghdour_2008,
  doi = {10.1371/journal.pgen.1000052},
  url = {https://dx.doi.org/10.1371/journal.pgen.1000052},
  year = {2008},
  month = {apr},
  publisher = {Public Library of Science ({PLoS})},
  volume = {4},
  number = {4},
  pages = {e1000052},
  author = {Youssef Idaghdour and John D. Storey and Sami J. Jadallah and Greg Gibson},
  editor = {Greg Barsh},
  title = {A genome-wide gene expression signature of environmental geography in leukocytes of Moroccan Amazighs},
  journal = {{PLoS} Genetics},
  abstract = {The different environments that humans experience are likely to impact physiology and disease susceptibility. In order to estimate the magnitude of the impact of environment on transcript abundance, we examined gene expression in peripheral blood leukocyte samples from 46 desert nomadic, mountain agrarian and coastal urban Moroccan Amazigh individuals. Despite great expression heterogeneity in humans, as much as one third of the leukocyte transcriptome was found to be associated with differences among regions. Genome-wide polymorphism analysis indicates that genetic differentiation in the total sample is limited and is unlikely to explain the expression divergence. Methylation profiling of 1,505 CpG sites suggests limited contribution of methylation to the observed differences in gene expression. Genetic network analysis further implies that specific aspects of immune function are strongly affected by regional factors and may influence susceptibility to respiratory and inflammatory disease. Our results show a strong genome-wide gene expression signature of regional population differences that presumably include lifestyle, geography, and biotic factors, implying that these can play at least as great a role as genetic divergence in modulating gene expression variation in humans.},
  date = {2008-04-11},
}

@Article{Kall_2008_msms,
  doi = {10.1021/pr700600n},
  url = {https://dx.doi.org/10.1021/pr700600n},
  year = {2008},
  month = {jan},
  publisher = {American Chemical Society ({ACS})},
  volume = {7},
  number = {1},
  pages = {29--34},
  author = {Lukas K{\"a}ll and John D. Storey and Michael J. MacCoss and William Stafford Noble},
  title = {Assigning significance to peptides identified by tandem mass spectrometry using decoy databases},
  journal = {Journal of Proteome Research},
  abstract = {Automated methods for assigning peptides to observed tandem mass spectra typically return a list of peptide−spectrum matches, ranked according to an arbitrary score. In this article, we describe methods for converting these arbitrary scores into more useful statistical significance measures. These methods employ a decoy sequence database as a model of the null hypothesis, and use false discovery rate (FDR) analysis to correct for multiple testing. We first describe a simple FDR inference method and then describe how estimating and taking into account the percentage of incorrectly identified spectra in the entire data set can lead to increased statistical power.},
  date = {2008-01-01},
}

@Article{Kall_2008_coin,
  doi = {10.1021/pr700739d},
  url = {https://dx.doi.org/10.1021/pr700739d},
  year = {2008},
  month = {jan},
  publisher = {American Chemical Society ({ACS})},
  volume = {7},
  number = {1},
  pages = {40--44},
  author = {Lukas K{\"a}ll and John D. Storey and Michael J. MacCoss and William Stafford Noble},
  title = {Posterior error probabilities and false discovery rates: two sides of the same coin},
  journal = {Journal of Proteome Research},
  abstract = {A variety of methods have been described in the literature for assigning statistical significance to peptides identified via tandem mass spectrometry. Here, we explain how two types of scores, the q-value and the posterior error probability, are related and complementary to one another.},
  date = {2008-01-01},
}

@Article{Kall_2008_pep,
  doi = {10.1093/bioinformatics/btn294},
  url = {https://dx.doi.org/10.1093/bioinformatics/btn294},
  year = {2008},
  month = {aug},
  publisher = {Oxford University Press ({OUP})},
  volume = {24},
  number = {16},
  pages = {i42--i48},
  author = {L. Kall and John D. Storey and W. S. Noble},
  title = {Non-parametric estimation of posterior error probabilities associated with peptides identified by tandem mass spectrometry},
  journal = {Bioinformatics},
  abstract = {Motivation: A mass spectrum produced via tandem mass spectrometry can be tentatively matched to a peptide sequence via database search. Here, we address the problem of assigning a posterior error probability (PEP) to a given peptide-spectrum match (PSM). This problem is considerably more difficult than the related problem of estimating the error rate associated with a large collection of PSMs. Existing methods for estimating PEPs rely on a parametric or semiparametric model of the underlying score distribution. Results: We demonstrate how to apply non-parametric logistic regression to this problem. The method makes no explicit assumptions about the form of the underlying score distribution; instead, the method relies upon decoy PSMs, produced by searching the spectra against a decoy sequence database, to provide a model of the null score distribution. We show that our non-parametric logistic regression method produces accurate PEP estimates for six different commonly used PSM score functions. In particular, the estimates produced by our method are comparable in accuracy to those of PeptideProphet, which uses a parametric or semiparametric model designed specifically to work with SEQUEST. The advantage of the non-parametric approach is applicability and robustness to new score functions and new types of data.},
  date = {2008-08-09},
}

@Article{Leek_2008,
  doi = {10.1073/pnas.0808709105},
  url = {https://dx.doi.org/10.1073/pnas.0808709105},
  year = {2008},
  month = {nov},
  publisher = {Proceedings of the National Academy of Sciences},
  volume = {105},
  number = {48},
  pages = {18718--18723},
  author = {J. T. Leek and John D. Storey},
  title = {A general framework for multiple testing dependence},
  journal = {Proceedings of the National Academy of Sciences},
  abstract = {We develop a general framework for performing large-scale significance testing in the presence of arbitrarily strong dependence. We derive a low-dimensional set of random vectors, called a dependence kernel, that fully captures the dependence structure in an observed high-dimensional dataset. This result shows a surprising reversal of the "curse of dimensionality" in the high-dimensional hypothesis testing setting. We show theoretically that conditioning on a dependence kernel is sufficient to render statistical tests independent regardless of the level of dependence in the observed data. This framework for multiple testing dependence has implications in a variety of common multiple testing problems, such as in gene expression studies, brain imaging, and spatial epidemiology.},
  date = {2008-12-02},
}

@Article{Schadt_2008,
  doi = {10.1371/journal.pbio.0060107},
  url = {https://dx.doi.org/10.1371/journal.pbio.0060107},
  year = {2008},
  month = {may},
  publisher = {Public Library of Science ({PLoS})},
  volume = {6},
  number = {5},
  pages = {e107},
  author = {Eric E Schadt and Cliona Molony and Eugene Chudin and Ke Hao and Xia Yang and Pek Y Lum and Andrew Kasarskis and Bin Zhang and Susanna Wang and Christine Suver and Jun Zhu and Joshua Millstein and Solveig Sieberts and John Lamb and Debraj GuhaThakurta and Jonathan Derry and John D. Storey and Iliana Avila-Campillo and Mark J Kruger and Jason M Johnson and Carol A Rohl and Atila {van Nas} and Margarete Mehrabian and Thomas A Drake and Aldons J Lusis and Ryan C Smith and F. Peter Guengerich and Stephen C Strom and Erin Schuetz and Thomas H Rushmore and Roger Ulrich},
  editor = {Goncalo Abecassis},
  title = {Mapping the genetic architecture of gene expression in human liver},
  journal = {{PLoS} Biology},
  abstract = {Genetic variants that are associated with common human diseases do not lead directly to disease, but instead act on intermediate, molecular phenotypes that in turn induce changes in higher-order disease traits. Therefore, identifying the molecular phenotypes that vary in response to changes in DNA and that also associate with changes in disease traits has the potential to provide the functional information required to not only identify and validate the susceptibility genes that are directly affected by changes in DNA, but also to understand the molecular networks in which such genes operate and how changes in these networks lead to changes in disease traits. Toward that end, we profiled more than 39,000 transcripts and we genotyped 782,476 unique single nucleotide polymorphisms (SNPs) in more than 400 human liver samples to characterize the genetic architecture of gene expression in the human liver, a metabolically active tissue that is important in a number of common human diseases, including obesity, diabetes, and atherosclerosis. This genome-wide association study of gene expression resulted in the detection of more than 6,000 associations between SNP genotypes and liver gene expression traits, where many of the corresponding genes identified have already been implicated in a number of human diseases. The utility of these data for elucidating the causes of common human diseases is demonstrated by integrating them with genotypic and expression data from other human and mouse populations. This provides much-needed functional support for the candidate susceptibility genes being identified at a growing number of genetic loci that have been identified as key drivers of disease from genome-wide association studies of disease. By using an integrative genomics approach, we highlight how the gene RPS26 and not ERBB3 is supported by our data as the most likely susceptibility gene for a novel type 1 diabetes locus recently identified in a large-scale, genome-wide association study. We also identify SORT1 and CELSR2 as candidate susceptibility genes for a locus recently associated with coronary artery disease and plasma low-density lipoprotein cholesterol levels in the process.},
  date = {2008-05-06},
}

%% 2009

@Article{Kall_2009,
  doi = {10.1093/bioinformatics/btp021},
  url = {https://dx.doi.org/10.1093/bioinformatics/btp021},
  year = {2009},
  month = {feb},
  publisher = {Oxford University Press ({OUP})},
  volume = {25},
  number = {7},
  pages = {964--966},
  author = {L. Kall and John D. Storey and W. S. Noble},
  title = {{QVALITY}: Non-parametric estimation of q-values and posterior error probabilities},
  journal = {Bioinformatics},
  abstract = {QVALITY is a C++ program for estimating two types of standard statistical confidence measures: the q-value, which is an analog of the p-value that incorporates multiple testing correction, and the posterior error probability (PEP, also known as the local false discovery rate), which corresponds to the probability that a given observation is drawn from the null distribution. In computing q-values, QVALITY employs a standard bootstrap procedure to estimate the prior probability of a score being from the null distribution; for PEP estimation, QVALITY relies upon non-parametric logistic regression. Relative to other tools for estimating statistical confidence measures, QVALITY is unique in its ability to estimate both types of scores directly from a null distribution, without requiring the user to calculate p-values.},
  date = {2009-02-04},
}

@Article{Kruglyak_2009,
  doi = {10.1038/nbt0609-544},
  url = {https://dx.doi.org/10.1038/nbt0609-544},
  year = {2009},
  month = {jun},
  publisher = {Springer Nature},
  volume = {27},
  number = {6},
  pages = {544--545},
  author = {Leonid Kruglyak and John D Storey},
  title = {Cause and express},
  journal = {Nature Biotechnology},
  abstract = {Biological validation of a cadre of new obesity genes supports the power of studies that exploit 'expression quantitative trait loci'.},
  date = {2009-06-01},
}

%% 2010

@Article{Mecham_2010,
  doi = {10.1093/bioinformatics/btq118},
  url = {https://dx.doi.org/10.1093/bioinformatics/btq118},
  year = {2010},
  month = {mar},
  publisher = {Oxford University Press ({OUP})},
  volume = {26},
  number = {10},
  pages = {1308--1315},
  author = {B. H. Mecham and P. S. Nelson and John D. Storey},
  title = {Supervised normalization of microarrays},
  journal = {Bioinformatics},
  abstract = {Motivation: A major challenge in utilizing microarray technologies to measure nucleic acid abundances is normalization, the goal of which is to separate biologically meaningful signal from other confounding sources of signal, often due to unavoidable technical factors. It is intuitively clear that true biological signal and confounding factors need to be simultaneously considered when performing normalization. However, the most popular normalization approaches do not utilize what is known about the study, both in terms of the biological variables of interest and the known technical factors in the study, such as batch or array processing date. Results: We show here that failing to include all study-specific biological and technical variables when performing normalization leads to biased downstream analyses. We propose a general normalization framework that fits a study-specific model employing every known variable that is relevant to the expression study. The proposed method is generally applicable to the full range of existing probe designs, as well as to both single-channel and dual-channel arrays. We show through real and simulated examples that the method has favorable operating characteristics in comparison to some of the most highly used normalization methods.},
  date = {2010-03-31},
}

@Article{Gresham_2010,
  doi = {10.1534/genetics.110.120766},
  url = {https://dx.doi.org/10.1534/genetics.110.120766},
  year = {2010},
  month = {oct},
  publisher = {Genetics Society of America},
  volume = {187},
  number = {1},
  pages = {299--317},
  author = {D. Gresham and V. M. Boer and A. Caudy and N. Ziv and N. J. Brandt and John D. Storey and D. Botstein},
  title = {System-level analysis of genes and functions affecting survival during nutrient starvation in Saccharomyces cerevisiae},
  journal = {Genetics},
  abstract = {An essential property of all cells is the ability to exit from active cell division and persist in a quiescent state. For single-celled microbes this primarily occurs in response to nutrient deprivation. We studied the genetic requirements for survival of Saccharomyces cerevisiae when starved for either of two nutrients: phosphate or leucine. We measured the survival of nearly all nonessential haploid null yeast mutants in mixed populations using a quantitative sequencing method that estimates the abundance of each mutant on the basis of frequency of unique molecular barcodes. Starvation for phosphate results in a population half-life of 337 hr whereas starvation for leucine results in a half-life of 27.7 hr. To measure survival of individual mutants in each population we developed a statistical framework that accounts for the multiple sources of experimental variation. From the identities of the genes in which mutations strongly affect survival, we identify genetic evidence for several cellular processes affecting survival during nutrient starvation, including autophagy, chromatin remodeling, mRNA processing, and cytoskeleton function. In addition, we found evidence that mitochondrial and peroxisome function is required for survival. Our experimental and analytical methods represent an efficient and quantitative approach to characterizing genetic functions and networks with unprecedented resolution and identified genotype-by-environment interactions that have important implications for interpretation of studies of aging and quiescence in yeast.},
  date = {2010-10-13},
}

@Article{Woo_2010,
  doi = {10.1093/bioinformatics/btq701},
  url = {https://dx.doi.org/10.1093/bioinformatics/btq701},
  year = {2010},
  month = {dec},
  publisher = {Oxford University Press ({OUP})},
  volume = {27},
  number = {4},
  pages = {509--515},
  author = {S. Woo and J. T. Leek and John D. Storey},
  title = {A computationally efficient modular optimal discovery procedure},
  journal = {Bioinformatics},
  abstract = {Motivation: It is well known that patterns of differential gene expression across biological conditions are often shared by many genes, particularly those within functional groups. Taking advantage of these patterns can lead to increased statistical power and biological clarity when testing for differential expression in a microarray experiment. The optimal discovery procedure (ODP), which maximizes the expected number of true positives for each fixed number of expected false positives, is a framework aimed at this goal. Storey et al. introduced an estimator of the ODP for identifying differentially expressed genes. However, their ODP estimator grows quadratically in computational time with respect to the number of genes. Reducing this computational burden is a key step in making the ODP practical for usage in a variety of high-throughput problems. Results: Here, we propose a new estimate of the ODP called the modular ODP (mODP). The existing 'full ODP' requires that the likelihood function for each gene be evaluated according to the parameter estimates for all genes. The mODP assigns genes to modules according to a Kullback–Leibler distance, and then evaluates the statistic only at the module-averaged parameter estimates. We show that the mODP is relatively insensitive to the choice of the number of modules, but dramatically reduces the computational complexity from quadratic to linear in the number of genes. We compare the full ODP algorithm and mODP on simulated data and gene expression data from a recent study of Morrocan Amazighs. The mODP and full ODP algorithm perform very similarly across a range of comparisons.},
  date = {2010-12-24},
}

%% 2011

@Article{Desai_2011,
  doi = {10.1371/journal.pmed.1001093},
  url = {https://dx.doi.org/10.1371/journal.pmed.1001093},
  year = {2011},
  month = {sep},
  publisher = {Public Library of Science ({PLoS})},
  volume = {8},
  number = {9},
  pages = {e1001093},
  author = {Keyur H. Desai and Chuen Seng Tan and Jeffrey T. Leek and Ronald V. Maier and Ronald G. Tompkins and John D. Storey},
  title = {Dissecting inflammatory complications in critically injured patients by within-patient gene expression changes: A longitudinal clinical genomics study},
  journal = {{PLoS} Medicine},
  abstract = {Background: Trauma is the number one killer of individuals 1–44 y of age in the United States. The prognosis and treatment of inflammatory complications in critically injured patients continue to be challenging, with a history of failed clinical trials and poorly understood biology. New approaches are therefore needed to improve our ability to diagnose and treat this clinical condition. Methods and Findings: We conducted a large-scale study on 168 blunt-force trauma patients over 28 d, measuring ∼400 clinical variables and longitudinally profiling leukocyte gene expression with ∼800 microarrays. Marshall MOF (multiple organ failure) clinical score trajectories were first utilized to organize the patients into five categories of increasingly poor outcomes. We then developed an analysis framework modeling early within-patient expression changes to produce a robust characterization of the genomic response to trauma. A quarter of the genome shows early expression changes associated with longer-term post-injury complications, captured by at least five dynamic co-expression modules of functionally related genes. In particular, early down-regulation of MHC-class II genes and up-regulation of p38 MAPK signaling pathway were found to strongly associate with longer-term post-injury complications, providing discrimination among patient outcomes from expression changes during the 40–80 h window post-injury. Conclusions: The genomic characterization provided here substantially expands the scope by which the molecular response to trauma may be characterized and understood. These results may be instrumental in furthering our understanding of the disease process and identifying potential targets for therapeutic intervention. Additionally, the quantitative approach we have introduced is potentially applicable to future genomics studies of rapidly progressing clinical conditions.},
  date = {2011-09-13},
}

@Article{Kanodia_2011,
  doi = {10.1242/dev.071571},
  url = {https://dx.doi.org/10.1242/dev.071571},
  year = {2011},
  month = {oct},
  publisher = {The Company of Biologists},
  volume = {138},
  number = {22},
  pages = {4867--4874},
  author = {J. S. Kanodia and Y. Kim and R. Tomer and Z. Khan and K. Chung and John D. Storey and H. Lu and P. J. Keller and S. Y. Shvartsman},
  title = {A computational statistics approach for estimating the spatial range of morphogen gradients},
  journal = {Development},
  abstract = {A crucial issue in studies of morphogen gradients relates to their range: the distance over which they can act as direct regulators of cell signaling, gene expression and cell differentiation. To address this, we present a straightforward statistical framework that can be used in multiple developmental systems. We illustrate the developed approach by providing a point estimate and confidence interval for the spatial range of the graded distribution of nuclear Dorsal, a transcription factor that controls the dorsoventral pattern of the Drosophila embryo.},
  date = {2011-10-17},
}

@Article{Leek_2011,
  doi = {10.2202/1544-6115.1673},
  url = {https://dx.doi.org/10.2202/1544-6115.1673},
  year = {2011},
  month = {Jun},
  publisher = {Walter de Gruyter {GmbH}},
  volume = {10},
  number = {1},
  author = {Jeffrey T Leek and John D. Storey},
  title = {The joint null criterion for multiple hypothesis tests},
  journal = {Statistical Applications in Genetics and Molecular Biology},
  abstract = {Simultaneously performing many hypothesis tests is a problem commonly encountered in high-dimensional biology. In this setting, a large set of p-values is calculated from many related features measured simultaneously. Classical statistics provides a criterion for defining what a “correct” p-value is when performing a single hypothesis test. We show here that even when each p-value is marginally correct under this single hypothesis criterion, it may be the case that the joint behavior of the entire set of p-values is problematic. On the other hand, there are cases where each p-value is marginally incorrect, yet the joint distribution of the set of p-values is satisfactory. Here, we propose a criterion defining a well behaved set of simultaneously calculated p-values that provides precise control of common error rates and we introduce diagnostic procedures for assessing whether the criterion is satisfied with simulations. Multiple testing p-values that satisfy our new criterion avoid potentially large study specific errors, but also satisfy the usual assumptions for strong control of false discovery rates and family-wise error rates. We utilize the new criterion and proposed diagnostics to investigate two common issues in high-dimensional multiple testing for genomics: dependent multiple hypothesis tests and pooled versus test-specific null distributions.},
  date = {2011-06-01},
}

@InCollection{storey2011false,
  doi = {10.1007/978-3-642-04898-2_248},
  url = {https://dx.doi.org/10.1007/978-3-642-04898-2_248},
  year  = {2011},
  publisher = {Springer Nature},
  pages = {504--508},
  author = {John D. Storey},
  title = {False discovery rate},
  editor = {Miodrag Lovric},
  booktitle = {International Encyclopedia of Statistical Science},
  abstract = {An encyclopedia entry on the false discovery rate.},
  date = {2011-01-01},
}

@Article{Xiao_2011,
  doi = {10.1084/jem.20111354},
  url = {https://dx.doi.org/10.1084/jem.20111354},
  year = {2011},
  month = {nov},
  publisher = {Rockefeller University Press},
  volume = {208},
  number = {13},
  pages = {2581--2590},
  author = {Wenzhong Xiao and Michael N. Mindrinos and Junhee Seok and Joseph Cuschieri and Alex G. Cuenca and Hong Gao and Douglas L. Hayden and Laura Hennessy and Ernest E. Moore and Joseph P. Minei and Paul E. Bankey and Jeffrey L. Johnson and Jason Sperry and Avery B. Nathens and Timothy R. Billiar and Michael A. West and Bernard H. Brownstein and Philip H. Mason and Henry V. Baker and Celeste C. Finnerty and Marc G. Jeschke and M. Cecilia L{\a'o}pez and Matthew B. Klein and Richard L. Gamelli and Nicole S. Gibran and Brett Arnoldo and Weihong Xu and Yuping Zhang and Steven E. Calvano and Grace P. McDonald-Smith and David A. Schoenfeld and John D. Storey and J. Perren Cobb and H. Shaw Warren and Lyle L. Moldawer and David N. Herndon and Stephen F. Lowry and Ronald V. Maier and Ronald W. Davis and Ronald G. Tompkins},
  title = {A genomic storm in critically injured humans},
  journal = {The Journal of Experimental Medicine},
  abstract = {Human survival from injury requires an appropriate inflammatory and immune response. We describe the circulating leukocyte transcriptome after severe trauma and burn injury, as well as in healthy subjects receiving low-dose bacterial endotoxin, and show that these severe stresses produce a global reprioritization affecting >80% of the cellular functions and pathways, a truly unexpected “genomic storm.” In severe blunt trauma, the early leukocyte genomic response is consistent with simultaneously increased expression of genes involved in the systemic inflammatory, innate immune, and compensatory antiinflammatory responses, as well as in the suppression of genes involved in adaptive immunity. Furthermore, complications like nosocomial infections and organ failure are not associated with any genomic evidence of a second hit and differ only in the magnitude and duration of this genomic reprioritization. The similarities in gene expression patterns between different injuries reveal an apparently fundamental human response to severe inflammatory stress, with genomic signatures that are surprisingly far more common than different. Based on these transcriptional data, we propose a new paradigm for the human immunological response to severe injury.},
  date = {2011-11-21},
}

@Article{Xu_2011,
  doi = {10.1073/pnas.1019753108},
  url = {https://dx.doi.org/10.1073/pnas.1019753108},
  year = {2011},
  month = {Mar},
  publisher = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {9},
  pages = {3707--3712},
  author = {W. Xu and J. Seok and M. N. Mindrinos and A. C. Schweitzer and H. Jiang and J. Wilhelmy and T. A. Clark and K. Kapur and Y. Xing and M. Faham and John D. Storey and L. L. Moldawer and R. V. Maier and R. G. Tompkins and W. H. Wong and R. W. Davis and W. Xiao and M. Toner and S. Warren and D. A. Schoenfeld and L. G. Rahme and G. P. McDonald-Smith and D. L. Hayden and P. H. Mason and S. Fagan and Y.-M. Yu and J. P. Cobb and D. G. Remick and J. A. Mannick and J. A. Lederer and R. L. Gamelli and G. M. Silver and M. A. West and M. B. Shapiro and R. D. Smith and D. G. Camp and W. Qian and R. Tibshirani and S. F. Lowry and S. E. Calvano and I. Chaudry and M. Cohen and E. E. Moore and J. L. Johnson and H. V. Baker and P. A. Efron and U. G. J. Balis and T. R. Billiar and J. B. Ochoa and J. Sperry and C. L. Miller-Graziano and A. K. De and P. E. Bankey and D. N. Herndon and C. C. Finnerty and M. G. Jeschke and J. P. Minei and B. D. Arnoldo and J. L. Hunt and J. Horton and B. H. Brownstein and B. Freeman and A. B. Nathens and J. Cuschieri and N. Gibran and M. Klein and G. O\textquotesingleKeefe and L. Altstein and H. Gao and B. G. Harbrecht and L. Hennessy and S. E. Honari and B. A. McKinley and F. A. Moore and B. Wispelwey and},
  title = {Human transcriptome array for high-throughput clinical studies},
  journal = {Proceedings of the National Academy of Sciences},
  abstract = {A 6.9 million-feature oligonucleotide array of the human transcriptome [Glue Grant human transcriptome (GG-H array)] has been developed for high-throughput and cost-effective analyses in clinical studies. This array allows comprehensive examination of gene expression and genome-wide identification of alternative splicing as well as detection of coding SNPs and noncoding transcripts. The performance of the array was examined and compared with mRNA sequencing (RNA-Seq) results over multiple independent replicates of liver and muscle samples. Compared with RNA-Seq of 46 million uniquely mappable reads per replicate, the GG-H array is highly reproducible in estimating gene and exon abundance. Although both platforms detect similar expression changes at the gene level, the GG-H array is more sensitive at the exon level. Deeper sequencing is required to adequately cover low-abundance transcripts. The array has been implemented in a multicenter clinical program and has generated high-quality, reproducible data. Considering the clinical trial requirements of cost, sample availability, and throughput, the GG-H array has a wide range of applications. An emerging approach for large-scale clinical genomic studies is to first use RNA-Seq to the sufficient depth for the discovery of transcriptome elements relevant to the disease process followed by high-throughput and reliable screening of these elements on thousands of patient samples using custom-designed arrays.},
  date = {2011-03-01},
}

%% 2012

@Article{1210.3313,
  author = {Troels T. Marstrand and John D. Storey},
  title = {Identifying and mapping cell-type specific chromatin programming of gene expression},
  year = {2012},
  journal = {arXiv},
  volume = {1210.3313},
  doi = {10.1073/pnas.1312523111},
  url = {https://arxiv.org/abs/1210.3313},
  abstract = {A problem of substantial interest is to systematically map variation in chromatin structure to gene expression regulation across conditions, environments, or differentiated cell types. We developed and applied a quantitative framework for determining the existence, strength, and type of relationship between high-resolution chromatin structure in terms of DNaseI hypersensitivity (DHS) and genome-wide gene expression levels in 20 diverse human cell lines. We show that ~25% of genes show cell-type specific expression explained by alterations in chromatin structure. We find that distal regions of chromatin structure (e.g., +/- 200kb) capture more genes with this relationship than local regions (e.g., +/- 2.5kb), yet the local regions show a more pronounced effect. By exploiting variation across cell-types, we were capable of pinpointing the most likely hypersensitive sites related to cell-type specific expression, which we show have a range of contextual usages. This quantitative framework is likely applicable to other settings aimed at relating continuous genomic measurements to gene expression variation.},
  date = {2011-12-12},
}

@Article{Desai_2012,
  doi = {10.1080/01621459.2011.645777},
  url = {https://dx.doi.org/10.1080/01621459.2011.645777},
  year = {2012},
  month = {mar},
  publisher = {Informa {UK} Limited},
  volume = {107},
  number = {497},
  pages = {135--151},
  author = {Keyur H. Desai and John D. Storey},
  title = {Cross-dimensional inference of dependent high-dimensional data},
  journal = {Journal of the American Statistical Association},
  abstract = {A growing number of modern scientific problems in areas such as genomics, neurobiology, and spatial epidemiology involve the measurement and analysis of thousands of related features that may be stochastically dependent at arbitrarily strong levels. In this work, we consider the scenario where the features follow a multivariate Normal distribution. We demonstrate that dependence is manifested as random variation shared among features, and that standard methods may yield highly unstable inference due to dependence, even when the dependence is fully parameterized and utilized in the procedure. We propose a 'cross-dimensional inference' framework that alleviates the problems due to dependence by modeling and removing the variation shared among features, while also properly regularizing estimation across features. We demonstrate the framework on both simultaneous point estimation and multiple hypothesis testing in scenarios derived from the scientific applications of interest.},
  date = {2012-03-01},
}

@Article{Leek_2012,
  doi = {10.1093/bioinformatics/bts034},
  url = {https://dx.doi.org/10.1093/bioinformatics/bts034},
  year = {2012},
  month = {jan},
  publisher = {Oxford University Press ({OUP})},
  volume = {28},
  number = {6},
  pages = {882--883},
  author = {J. T. Leek and W. E. Johnson and H. S. Parker and A. E. Jaffe and John D. Storey},
  title = {The sva package for removing batch effects and other unwanted variation in high-throughput experiments},
  journal = {Bioinformatics},
  abstract = {Heterogeneity and latent variables are now widely recognized as major sources of bias and variability in high-throughput experiments. The most well-known source of latent variation in genomic experiments are batch effects—when samples are processed on different days, in different groups or by different people. However, there are also a large number of other variables that may have a major impact on high-throughput measurements. Here we describe the sva package for identifying, estimating and removing unwanted sources of variation in high-throughput experiments. The sva package supports surrogate variable estimation with the sva function, direct adjustment for known batch effects with the ComBat function and adjustment for batch and latent variables in prediction problems with the fsva function.},
  date = {2012-01-17},
}

%% 2013

@Article{1308.6013,
  author = {Neo Christopher Chung and John D. Storey},
  title = {Statistical significance of variables driving systematic variation},
  year = {2013},
  journal = {arXiv},
  volume = {1308.6013},
  doi = {10.1093/bioinformatics/btu674},
  url ={https://arxiv.org/abs/1308.6013},
  abstract = {There are a number of well-established methods such as principal components analysis (PCA) for automatically capturing systematic variation due to latent variables in large-scale genomic data. PCA and related methods may directly provide a quantitative characterization of a complex biological variable that is otherwise difficult to precisely define or model. An unsolved problem in this context is how to systematically identify the genomic variables that are drivers of systematic variation captured by PCA. Principal components (and other estimates of systematic variation) are directly constructed from the genomic variables themselves, making measures of statistical significance artificially inflated when using conventional methods due to over-fitting. We introduce a new approach called the jackstraw that allows one to accurately identify genomic variables that are statistically significantly associated with any subset or linear combination of principal components (PCs). The proposed method can greatly simplify complex significance testing problems encountered in genomics and can be utilized to identify the genomic variables significantly associated with latent variables. Using simulation, we demonstrate that our method attains accurate measures of statistical significance over a range of relevant scenarios. We consider yeast cell-cycle gene expression data, and show that the proposed method can be used to straightforwardly identify statistically significant genes that are cell-cycle regulated. We also analyze gene expression data from post-trauma patients, allowing the gene expression data to provide a molecularly-driven phenotype. We find a greater enrichment for inflammatory-related gene sets compared to using a clinically defined phenotype. The proposed method provides a useful bridge between large-scale quantifications of systematic variation and gene-level significance analyses.},
  date = {2013-08-27},
}

@Article{1312.2041,
  author = {Wei Hao and Minsun Song and John D. Storey},
  title = {Probabilistic models of genetic variation in structured populations applied to global human studies},
  year = {2013},
  journal = {arXiv},
  volume = {1312.2041},
  doi = {10.1093/bioinformatics/btv641},
  url = {https://arxiv.org/abs/1312.2041},
  abstract = {Modern population genetics studies typically involve genome-wide genotyping of individuals from a diverse network of ancestries. An important, unsolved problem is how to formulate and estimate probabilistic models of observed genotypes that allow for complex population structure. We formulate two general probabilistic models, and we propose computationally efficient algorithms to estimate them. First, we show how principal component analysis (PCA) can be utilized to estimate a general model that includes the well-known Pritchard-Stephens-Donnelly mixed-membership model as a special case. Noting some drawbacks of this approach, we introduce a new 'logistic factor analysis' (LFA) framework that seeks to directly model the logit transformation of probabilities underlying observed genotypes in terms of latent variables that capture population structure. We demonstrate these advances on data from the human genome diversity panel and 1000 genomes project, where we are able to identify SNPs that are highly differentiated with respect to structure while making minimal modeling assumptions.},
  date = {2013-12-07},
}

@Article{1301.3933,
  author = {Andrew E. Jaffe and John D. Storey and Hongkai Ji and Jeffrey T. Leek},
  title = {Gene set bagging for estimating replicability of gene set analyses},
  year = {2013},
  journal = {arXiv},
  volume = {1301.3933},
  doi = {10.1186/1471-2105-14-360},
  url = {https://arxiv.org/abs/1301.3933},
  abstract = {Background: Significance analysis plays a major role in identifying and ranking genes, transcription factor binding sites, DNA methylation regions, and other high-throughput features for association with disease. We propose a new approach, called gene set bagging, for measuring the stability of ranking procedures using predefined gene sets. Gene set bagging involves resampling the original high-throughput data, performing gene-set analysis on the resampled data, and confirming that biological categories replicate. This procedure can be thought of as bootstrapping gene-set analysis and can be used to determine which are the most reproducible gene sets. Results: Here we apply this approach to two common genomics applications: gene expression and DNA methylation. Even with state-of-the-art statistical ranking procedures, significant categories in a gene set enrichment analysis may be unstable when subjected to resampling. Conclusions: We demonstrate that gene lists are not necessarily stable, and therefore additional steps like gene set bagging can improve biological inference of gene set analysis.},
  date = {2013-06-16},
}

@Article{Jaffe_2013,
  doi = {10.1186/1471-2105-14-360},
  url = {https://dx.doi.org/10.1186/1471-2105-14-360},
  year = {2013},
  publisher = {Springer Nature},
  volume = {14},
  number = {1},
  pages = {360},
  author = {Andrew E. Jaffe and John D. Storey and Hongkai Ji and Jeffrey T. Leek},
  title = {Gene set bagging for estimating the probability a statistically significant result will replicate},
  journal = {{BMC} Bioinformatics},
  abstract = {Background: Significance analysis plays a major role in identifying and ranking genes, transcription factor binding sites, DNA methylation regions, and other high-throughput features associated with illness. We propose a new approach, called gene set bagging, for measuring the probability that a gene set replicates in future studies. Gene set bagging involves resampling the original high-throughput data, performing gene-set analysis on the resampled data, and confirming that biological categories replicate in the bagged samples. Results: Using both simulated and publicly-available genomics data, we demonstrate that significant categories in a gene set enrichment analysis may be unstable when subjected to resampling. We show our method estimates the replication probability (R), the probability that a gene set will replicate as a significant result in future studies, and show in simulations that this method reflects replication better than each set’s p-value. Conclusions: Our results suggest that gene lists based on p-values are not necessarily stable, and therefore additional steps like gene set bagging may improve biological inference on gene sets.},
  date = {2013-12-12},
}

@Article{Robinson_2013,
  doi = {10.1534/g3.113.008565},
  url = {https://dx.doi.org/10.1534/g3.113.008565},
  year = {2013},
  month = {nov},
  publisher = {Genetics Society of America},
  volume = {4},
  number = {1},
  pages = {11--18},
  author = {D. G. Robinson and W. Chen and John D. Storey and D. Gresham},
  title = {Design and analysis of bar-seq experiments},
  journal = {G3: Genes | Genomes | Genetics},
  abstract = {High-throughput quantitative DNA sequencing enables the parallel phenotyping of pools of thousands of mutants. However, the appropriate analytical methods and experimental design that maximize the efficiency of these methods while maintaining statistical power are currently unknown. Here, we have used Bar-seq analysis of the Saccharomyces cerevisiae yeast deletion library to systematically test the effect of experimental design parameters and sequence read depth on experimental results. We present computational methods that efficiently and accurately estimate effect sizes and their statistical significance by adapting existing methods for RNA-seq analysis. Using simulated variation of experimental designs, we found that biological replicates are critical for statistical analysis of Bar-seq data, whereas technical replicates are of less value. By subsampling sequence reads, we found that when using four-fold biological replication, 6 million reads per condition achieved 96% power to detect a two-fold change (or more) at a 5% false discovery rate. Our guidelines for experimental design and computational analysis enables the study of the yeast deletion collection in up to 30 different conditions in a single sequencing lane. These findings are relevant to a variety of pooled genetic screening methods that use high-throughput quantitative DNA sequencing, including Tn-seq.},
  date = {2013-11-05},
}

%% 2014

@Article{Robinson013342,
  author = {Robinson, David G. and Wang, Jean and Storey, John D.},
  title = {A nested parallel experiment demonstrates differences in intensity-dependence between RNA-seq and microarrays},
  year = {2014},
  doi = {10.1101/013342},
  publisher = {Cold Spring Harbor Labs Journals},
  url = {https://dx.doi.org/10.1101/013342},
  journal = {bioRxiv},
  volume = {doi:10.1101/013342},
  abstract = {Understanding the differences between microarray and RNA-Seq technologies for measuring gene expression is necessary for informed design of experiments and choice of data analysis methods. Previous comparisons have come to sometimes contradictory conclusions, which we suggest result from a lack of attention to the intensity-dependent nature of variation generated by the technologies. To examine this trend, we carried out a parallel nested experiment performed simultaneously on the two technologies that systematically split variation into four stages (treatment, biological variation, library preparation, and chip/lane noise), allowing a separation and comparison of the sources of variation in a well-controlled cellular system, Saccharomyces cerevisiae. With this novel dataset, we demonstrate that power and accuracy are more dependent on per-gene read depth in RNA-Seq than they are on fluorescence intensity in microarrays. However, we carried out qPCR validations which indicate that microarrays may demonstrate greater systematic bias in low-intensity genes than in RNA-seq.},
  date = {2014-12-30},
}

@Article{1409.6384,
  author = {Alejandro Ochoa and John D. Storey and Manuel Llinás and Mona Singh},
  title = {Beyond the E-value: Stratified statistics for protein domain prediction},
  year = {2014},
  journal = {arXiv},
  volume = {1409.6384},
  doi = {10.1371/journal.pcbi.1004509},
  url = {https://arxiv.org/abs/1409.6384},
  abstract = {E-values have been the dominant statistic for protein sequence analysis for the past two decades: from identifying statistically significant local sequence alignments to evaluating matches to hidden Markov models describing protein domain families. Here we formally show that for 'stratified' multiple hypothesis testing problems, controlling the local False Discovery Rate (lFDR) per stratum, or partition, yields the most predictions across the data at any given threshold on the FDR or E-value over all strata combined. For the important problem of protein domain prediction, a key step in characterizing protein structure, function and evolution, we show that stratifying statistical tests by domain family yields excellent results. We develop the first FDR-estimating algorithms for domain prediction, and evaluate how well thresholds based on q-values, E-values and lFDRs perform in domain prediction using five complementary approaches for estimating empirical FDRs in this context. We show that stratified q-value thresholds substantially outperform E-values. Contradicting our theoretical results, q-values also outperform lFDRs; however, our tests reveal a small but coherent subset of domain families, biased towards models for specific repetitive patterns, for which FDRs are greatly underestimated due to weaknesses in random sequence models. Usage of lFDR thresholds outperform q-values for the remaining families, which have as-expected noise, suggesting that further improvements in domain predictions can be achieved with improved modeling of random sequences. Overall, our theoretical and empirical findings suggest that the use of stratified q-values and lFDRs could result in improvements in a host of structured multiple hypothesis testing problems arising in bioinformatics, including genome-wide association studies, orthology prediction, motif scanning, and multi-microarray analyses.},
  date = {2014-09-23},
}

@Article{Kim_2014,
  doi = {10.1186/gm560},
  url = {https://dx.doi.org/10.1186/gm560},
  year = {2014},
  publisher = {Springer Nature},
  volume = {6},
  number = {5},
  pages = {40},
  author = {Jinhee Kim and Nima Ghasemzadeh and Danny J Eapen and Neo Chung and John D Storey and Arshed A Quyyumi and Greg Gibson},
  title = {Gene expression profiles associated with acute myocardial infarction and risk of cardiovascular death},
  journal = {Genome Medicine},
  abstract = {Background: Genetic risk scores have been developed for coronary artery disease and atherosclerosis, but are not predictive of adverse cardiovascular events. We asked whether peripheral blood expression profiles may be predictive of acute myocardial infarction (AMI) and/or cardiovascular death. Methods: Peripheral blood samples from 338 subjects aged 62 ± 11 years with coronary artery disease (CAD) were analyzed in two phases (discovery N = 175, and replication N = 163), and followed for a mean 2.4 years for cardiovascular death. Gene expression was measured on Illumina HT-12 microarrays with two different normalization procedures to control technical and biological covariates. Whole genome genotyping was used to support comparative genome-wide association studies of gene expression. Analysis of variance was combined with receiver operating curve and survival analysis to define a transcriptional signature of cardiovascular death. Results: In both phases, there was significant differential expression between healthy and AMI groups with overall down-regulation of genes involved in T-lymphocyte signaling and up-regulation of inflammatory genes. Expression quantitative trait loci analysis provided evidence for altered local genetic regulation of transcript abundance in AMI samples. On follow-up there were 31 cardiovascular deaths. A principal component (PC1) score capturing covariance of 238 genes that were differentially expressed between deceased and survivors in the discovery phase significantly predicted risk of cardiovascular death in the replication and combined samples (hazard ratio = 8.5, P < 0.0001) and improved the C-statistic (area under the curve 0.82 to 0.91, P = 0.03) after adjustment for traditional covariates. Conclusions: A specific blood gene expression profile is associated with a significant risk of death in Caucasian subjects with CAD. This comprises a subset of transcripts that are also altered in expression during acute myocardial infarction.},
  date = {2014-05-30},
}

@Article{Marstrand_2014,
  doi = {10.1073/pnas.1312523111},
  url = {https://dx.doi.org/10.1073/pnas.1312523111},
  year = {2014},
  month = {jan},
  publisher = {Proceedings of the National Academy of Sciences},
  volume = {111},
  number = {6},
  pages = {E645--E654},
  author = {Troels T. Marstrand and John D. Storey},
  title = {Identifying and mapping cell-type-specific chromatin programming of gene expression},
  journal = {Proceedings of the National Academy of Sciences},
  abstract = {A problem of substantial interest is to systematically map variation in chromatin structure to gene-expression regulation across conditions, environments, or differentiated cell types. We developed and applied a quantitative framework for determining the existence, strength, and type of relationship between high-resolution chromatin structure in terms of DNaseI hypersensitivity and genome-wide gene-expression levels in 20 diverse human cell types. We show that ∼25% of genes show cell-type-specific expression explained by alterations in chromatin structure. We find that distal regions of chromatin structure (e.g., +/-200 kb) capture more genes with this relationship than local regions (e.g., +/-2.5 kb), yet the local regions show a more pronounced effect. By exploiting variation across cell types, we were capable of pinpointing the most likely hypersensitive sites related to cell-type-specific expression, which we show have a range of contextual uses. This quantitative framework is likely applicable to other settings aimed at relating continuous genomic measurements to gene-expression variation.},
  date = {2014-01-27},
}

@Article{Chung_2014,
  doi = {10.1093/bioinformatics/btu674},
  url = {https://dx.doi.org/10.1093/bioinformatics/btu674},
  year = {2014},
  month = {oct},
  publisher = {Oxford University Press ({OUP})},
  volume = {31},
  number = {4},
  pages = {545--554},
  author = {N. C. Chung and John D. Storey},
  title = {Statistical significance of variables driving systematic variation in high-dimensional data},
  journal = {Bioinformatics},
  abstract = {Motivation: There are a number of well-established methods such as principal component analysis (PCA) for automatically capturing systematic variation due to latent variables in large-scale genomic data. PCA and related methods may directly provide a quantitative characterization of a complex biological variable that is otherwise difficult to precisely define or model. An unsolved problem in this context is how to systematically identify the genomic variables that are drivers of systematic variation captured by PCA. Principal components (PCs) (and other estimates of systematic variation) are directly constructed from the genomic variables themselves, making measures of statistical significance artificially inflated when using conventional methods due to over-fitting. Results: We introduce a new approach called the jackstraw that allows one to accurately identify genomic variables that are statistically significantly associated with any subset or linear combination of PCs. The proposed method can greatly simplify complex significance testing problems encountered in genomics and can be used to identify the genomic variables significantly associated with latent variables. Using simulation, we demonstrate that our method attains accurate measures of statistical significance over a range of relevant scenarios. We consider yeast cell-cycle gene expression data, and show that the proposed method can be used to straightforwardly identify genes that are cell-cycle regulated with an accurate measure of statistical significance. We also analyze gene expression data from post-trauma patients, allowing the gene expression data to provide a molecularly driven phenotype. Using our method, we find a greater enrichment for inflammatory-related gene sets compared to the original analysis that uses a clinically defined, although likely imprecise, phenotype. The proposed method provides a useful bridge between large-scale quantifications of systematic variation and gene-level significance analyses.},
  date = {2014-10-21},
}

@Article{Robinson_2014,
  doi = {10.1093/bioinformatics/btu552},
  url = {https://dx.doi.org/10.1093/bioinformatics/btu552},
  year = {2014},
  month = {sep},
  publisher = {Oxford University Press ({OUP})},
  volume = {30},
  number = {23},
  pages = {3424--3426},
  author = {D. G. Robinson and John D. Storey},
  title = {{subSeq}: Determining appropriate sequencing depth through efficient read subsampling},
  journal = {Bioinformatics},
  abstract = {Motivation: Next-generation sequencing experiments, such as RNA-Seq, play an increasingly important role in biological research. One complication is that the power and accuracy of such experiments depend substantially on the number of reads sequenced, so it is important and challenging to determine the optimal read depth for an experiment or to verify whether one has adequate depth in an existing experiment. Results: By randomly sampling lower depths from a sequencing experiment and determining where the saturation of power and accuracy occurs, one can determine what the most useful depth should be for future experiments, and furthermore, confirm whether an existing experiment had sufficient depth to justify its conclusions. We introduce the subSeq R package, which uses a novel efficient approach to perform this subsampling and to calculate informative metrics at each depth.},
  date = {2014-09-03},
}

%% 2015

@Article{Song012682,
  author = {Song, Minsun and Hao, Wei and Storey, John D.},
  title = {Testing for genetic associations in arbitrarily structured populations},
  year = {2015},
  doi = {10.1101/012682},
  publisher = {Cold Spring Harbor Labs Journals},
  url = {https://dx.doi.org/10.1101/012682},
  journal = {bioRxiv},
  volume = {doi:10.1101/012682},
  abstract = {We present a new statistical test of association between a trait (either quantitative or binary) and genetic markers, which we theoretically and practically prove to be robust to arbitrarily complex population structure. The statistical test involves a set of parameters that can be directly estimated from large-scale genotyping data, such as that measured in genome-wide associations studies (GWAS). We also derive a new set of methodologies, called a genotype-conditional association test (GCAT), shown to provide accurate association tests in populations with complex structures, manifested in both the genetic and environmental contributions to the trait. We demonstrate the proposed method on a large simulation study and on the Northern Finland Birth Cohort study. In the Finland study, we identify several new significant loci that other methods do not detect. Our proposed framework provides a substantially different approach to the problem from existing methods. We provide some discussion on its similarities and differences with the linear mixed model and principal component approaches.},
  date = {2015-03-04},
}

@Article{1510.03497,
  author = {Xiongzhi Chen and John D. Storey},
  title = {Consistent estimation of low-dimensional latent structure in high-dimensional data},
  year = {2015},
  journal = {arXiv},
  volume = {1510.03497},
  url = {https://arxiv.org/abs/1510.03497},
  abstract = {We consider the problem of extracting a low-dimensional, linear latent variable structure from high-dimensional random variables. Specifically, we show that under mild conditions and when this structure manifests itself as a linear space that spans the conditional means, it is possible to consistently recover the structure using only information up to the second moments of these random variables. This finding, specialized to one-parameter exponential families whose variance function is quadratic in their means, allows for the derivation of an explicit estimator of such latent structure. This approach serves as a latent variable model estimator and as a tool for dimension reduction for a high-dimensional matrix of data composed of many related variables. Our theoretical results are verified by simulation studies and an application to genomic data.},
  date = {2015-10-13},
}

@Article{Gopalan013227,
  author = {Gopalan, Prem and Hao, Wei and Blei, David M. and Storey, John D.},
  title = {Scaling probabilistic models of genetic variation to millions of humans},
  year = {2015},
  doi = {10.1101/013227},
  publisher = {Cold Spring Harbor Labs Journals},
  url = {https://dx.doi.org/10.1101/013227},
  journal = {bioRxiv},
  volume = {doi:10.1101/013227},
  abstract = {One of the major goals of population genetics is to quantitatively understand variation of genetic polymorphisms among individuals. To this end, researchers have developed sophisticated statistical methods to capture the complex population structure that underlies observed genotypes in humans, and such methods have been effective for analyzing modestly sized genomic data sets. However, the number of genotyped humans has grown significantly in recent years, and it is accelerating. In aggregate about 1M individuals have been genotyped to date. Analyzing these data will bring us closer to a nearly complete picture of human genetic variation; but existing methods for population genetics analysis do not scale to data of this size. To solve this problem we developed TeraStructure. TeraStructure is a new algorithm to fit Bayesian models of genetic variation in human populations on tera-sample-sized data sets ($10^12$ observed genotypes, e.g., 1M individuals at 1M SNPs). It is a principled approach to Bayesian inference that iterates between subsampling locations of the genome and updating an estimate of the latent population structure of the individuals. On data sets of up to 2K individuals, TeraStructure matches the existing state of the art in terms of both speed and accuracy. On simulated data sets of up to 10K individuals, TeraStructure is twice as fast as existing methods and has higher accuracy in recovering the latent population structure. On genomic data simulated at the tera-sample-size scales, TeraStructure continues to be accurate and is the only method that can complete its analysis.},
  date = {2015-05-28},
}

@Article{Ochoa_2015,
  doi = {10.1371/journal.pcbi.1004509},
  url = {https://dx.doi.org/10.1371/journal.pcbi.1004509},
  year = {2015},
  month = {nov},
  publisher = {Public Library of Science ({PLoS})},
  volume = {11},
  number = {11},
  pages = {e1004509},
  author = {Alejandro Ochoa and John D. Storey and Manuel Llin{\a'a}s and Mona Singh},
  title = {Beyond the E-Value: Stratified statistics for protein domain prediction},
  journal = {{PLOS} Computational Biology},
  abstract = {E-values have been the dominant statistic for protein sequence analysis for the past two decades: from identifying statistically significant local sequence alignments to evaluating matches to hidden Markov models describing protein domain families. Here we formally show that for 'stratified' multiple hypothesis testing problems—that is, those in which statistical tests can be partitioned naturally—controlling the local False Discovery Rate (lFDR) per stratum, or partition, yields the most predictions across the data at any given threshold on the FDR or E-value over all strata combined. For the important problem of protein domain prediction, a key step in characterizing protein structure, function and evolution, we show that stratifying statistical tests by domain family yields excellent results. We develop the first FDR-estimating algorithms for domain prediction, and evaluate how well thresholds based on q-values, E-values and lFDRs perform in domain prediction using five complementary approaches for estimating empirical FDRs in this context. We show that stratified q-value thresholds substantially outperform E-values. Contradicting our theoretical results, q-values also outperform lFDRs; however, our tests reveal a small but coherent subset of domain families, biased towards models for specific repetitive patterns, for which weaknesses in random sequence models yield notably inaccurate statistical significance measures. Usage of lFDR thresholds outperform q-values for the remaining families, which have as-expected noise, suggesting that further improvements in domain predictions can be achieved with improved modeling of random sequences. Overall, our theoretical and empirical findings suggest that the use of stratified q-values and lFDRs could result in improvements in a host of structured multiple hypothesis testing problems arising in bioinformatics, including genome-wide association studies, orthology prediction, and motif scanning.},
  date = {2015-11-17},
}

@Article{Robinson_2015,
  doi = {10.1093/nar/gkv636},
  url = {https://dx.doi.org/10.1093/nar/gkv636},
  year = {2015},
  month = {jun},
  publisher = {Oxford University Press ({OUP})},
  pages = {gkv636},
  author = {David G. Robinson and Jean Y. Wang and John D. Storey},
  title = {A nested parallel experiment demonstrates differences in intensity-dependence between {RNA}-seq and microarrays},
  journal = {Nucleic Acids Research},
  abstract = {Understanding the differences between microarray and RNA-Seq technologies for measuring gene expression is necessary for informed design of experiments and choice of data analysis methods. Previous comparisons have come to sometimes contradictory conclusions, which we suggest result from a lack of attention to the intensity-dependent nature of variation generated by the technologies. To examine this trend, we carried out a parallel nested experiment performed simultaneously on the two technologies that systematically split variation into four stages (treatment, biological variation, library preparation and chip/lane noise), allowing a separation and comparison of the sources of variation in a well-controlled cellular system, Saccharomyces cerevisiae. With this novel dataset, we demonstrate that power and accuracy are more dependent on per-gene read depth in RNA-Seq than they are on fluorescence intensity in microarrays. However, we carried out quantitative PCR validations which indicate that microarrays may demonstrate greater systematic bias in low-intensity genes than in RNA-seq.},
  date = {2015-06-30},
}

@Article{Song_2015,
  doi = {10.1038/ng.3244},
  url = {https://dx.doi.org/10.1038/ng.3244},
  year = {2015},
  month = {mar},
  publisher = {Springer Nature},
  volume = {47},
  number = {5},
  pages = {550--554},
  author = {Minsun Song and Wei Hao and John D. Storey},
  title = {Testing for genetic associations in arbitrarily structured populations},
  journal = {Nature Genetics},
  abstract = {We present a new statistical test of association between a trait and genetic markers, which we theoretically and practically prove to be robust to arbitrarily complex population structure. The statistical test involves a set of parameters that can be directly estimated from large-scale genotyping data, such as those measured in genome-wide association studies (GWAS). We also derive a new set of methodologies, called a 'genotype-conditional association test' (GCAT), shown to provide accurate association tests in populations with complex structures, manifested in both the genetic and non-genetic contributions to the trait. We demonstrate the proposed method on a large simulation study and on the Northern Finland Birth Cohort study. In the Finland study, we identify several new significant loci that other methods do not detect. Our proposed framework provides a substantially different approach to the problem from existing methods, such as the linear mixed-model and principal-component approaches.},
  date = {2015-03-30},
}

@Article{Hao_2015,
  doi = {10.1093/bioinformatics/btv641},
  url = {https://dx.doi.org/10.1093/bioinformatics/btv641},
  year = {2015},
  month = {nov},
  publisher = {Oxford University Press ({OUP})},
  volume = {32},
  number = {5},
  pages = {713--721},
  author = {Wei Hao and Minsun Song and John D. Storey},
  title = {Probabilistic models of genetic variation in structured populations applied to global human studies},
  journal = {Bioinformatics},
  abstract = {Motivation: Modern population genetics studies typically involve genome-wide genotyping of individuals from a diverse network of ancestries. An important problem is how to formulate and estimate probabilistic models of observed genotypes that account for complex population structure. The most prominent work on this problem has focused on estimating a model of admixture proportions of ancestral populations for each individual. Here, we instead focus on modeling variation of the genotypes without requiring a higher-level admixture interpretation. Results: We formulate two general probabilistic models, and we propose computationally efficient algorithms to estimate them. First, we show how principal component analysis can be utilized to estimate a general model that includes the well-known Pritchard–Stephens–Donnelly admixture model as a special case. Noting some drawbacks of this approach, we introduce a new ‘logistic factor analysis’ framework that seeks to directly model the logit transformation of probabilities underlying observed genotypes in terms of latent variables that capture population structure. We demonstrate these advances on data from the Human Genome Diversity Panel and 1000 Genomes Project, where we are able to identify SNPs that are highly differentiated with respect to structure while making minimal modeling assumptions.},
  date = {2015-11-06},
}

%% 2016

@Article{Hackett_2016,
  doi = {10.1126/science.aaf2786},
  url = {https://dx.doi.org/10.1126/science.aaf2786},
  year = {2016},
  month = {oct},
  publisher = {American Association for the Advancement of Science ({AAAS})},
  volume = {354},
  number = {6311},
  pages = {aaf2786--aaf2786},
  author = {S. R. Hackett and V. R. T. Zanotelli and W. Xu and J. Goya and J. O. Park and D. H. Perlman and P. A. Gibney and D. Botstein and John D. Storey and John D. Rabinowitz},
  title = {Systems-level analysis of mechanisms regulating yeast metabolic flux},
  journal = {Science},
  abstract = {INTRODUCTION: Metabolism is among the most strongly conserved processes across all domains of life and is crucial for both bioengineering and disease research, yet we still have an unclear understanding of how metabolic rates (fluxes) are determined. Qualitatively, this deficiency involves missing knowledge of enzyme regulators. Quantitatively, it involves limited understanding of the relative contributions of enzyme and metabolite concentrations in controlling flux across physiological conditions. Addressing these gaps has been challenging because in vitro biochemical approaches lack the physiological context, whereas models of cellular metabolic dynamics have limited capacity for identifying or quantitating specific regulatory events because of overall model complexity. RATIONALE: Flux through individual metabolic reactions is directly determined by the concentrations of enzyme, substrates, products, and any allosteric regulators, as captured quantitatively by a Michaelis-Menten–style reaction equation. Analogous to how experimental variation of reaction species in vitro allows for the inference of regulators and reaction equation kinetic parameters, physiological changes in flux entail a change in reaction species that can be used to determine reaction equations on the basis of cellular data. This requires measurement across multiple biological conditions of flux, enzyme concentrations, and metabolite concentrations. We reasoned that chemostat cultures could be used to induce predictable and strong flux changes, with changes in enzymes and metabolites measurable by proteomics and metabolomics. By directly relating cellular flux to the reaction species that determine it, we can carry out regulatory inference at the level of single metabolic reactions by using cellular data. An important benefit is that the physiological significance of any identified regulator is implicit from its role in determining cellular flux. RESULTS: Here we introduce systematic identification of meaningful metabolic enzyme regulation (SIMMER). We measured fluxes, and metabolite and enzyme concentrations, in each of 25 yeast chemostats. For each of 56 reactions for which the flux, enzyme, and substrates were measured, we determined whether variation in measured flux could be explained by simple Michaelis-Menten kinetics. We also evaluated alternative models of each reaction’s kinetics that included a suite of allosteric regulators drawn from across all organisms. For 46 reactions, we were able to identify a useful kinetic model, with 17 reactions not including any regulation and 29 reactions being regulated by one to two allosteric regulators. Three previously unrecognized cross-pathway regulatory interactions were validated biochemically. These included inhibition of pyruvate kinase by citrate and inhibition of pyruvate decarboxylase by phenypyruvate. These metabolites accumulated and thereby curtailed glycolytic outflow and ethanol production in nitrogen-limited yeast. For well-supported reaction forms, we were able to determine the extent to which nutrient-based changes in flux were determined by changes in the concentrations of individual reaction species. We find that substrates are the most important determinant of fluxes in general, with enzymes and allosteric regulators having a comparably important role in the case of physiologically irreversible reactions. CONCLUSION: By connecting changes in flux to their root cause, SIMMER parallels classic in vitro approaches, but it allows simultaneous testing of numerous regulators of many reactions under physiological conditions. Its application to yeast showed that changes in flux across nutrient conditions are predominantly due to metabolite, not enzyme, levels. Thus, yeast metabolism is substantially self-regulating.},
  date = {2016-10-28},
}

@Article{Gopalan2016,
  doi = {10.1038/ng.3710},
  url = {https://dx.doi.org/10.1038/ng.3710},
  year  = {2016},
  month = {nov},
  publisher = {Springer Nature},
  volume = {48},
  number = {12},
  pages = {1587--1590},
  author = {Prem Gopalan and Wei Hao and David M. Blei and John D. Storey},
  title = {Scaling probabilistic models of genetic variation to millions of humans},
  journal = {Nature Genetics},
  abstract = {A major goal of population genetics is to quantitatively understand variation of genetic polymorphisms among individuals. The aggregated number of genotyped humans is currently on the order of millions of individuals, and existing methods do not scale to data of this size. To solve this problem, we developed TeraStructure, an algorithm to fit Bayesian models of genetic variation in structured human populations on tera-sample-sized data sets (1012 observed genotypes; for example, 1 million individuals at 1 million SNPs). TeraStructure is a scalable approach to Bayesian inference in which subsamples of markers are used to update an estimate of the latent population structure among individuals. We demonstrate that TeraStructure performs as well as existing methods on current globally sampled data, and we show using simulations that TeraStructure continues to be accurate and is the only method that can scale to tera-sample sizes.},
  date = {2016-11-07},
}

@Article{Ochoa083915,
  author = {Ochoa, Alejandro and Storey, John D.},
  title = {$F_{ST}$ and kinship for arbitrary population structures I: Generalized definitions},
  year = {2016},
  doi = {10.1101/083915},
  publisher = {Cold Spring Harbor Labs Journals},
  url = {https://dx.doi.org/10.1101/083915},
  journal = {bioRxiv},
  volume = {doi:10.1101/083915},
  abstract = {$F_{ST}$ is a fundamental measure of genetic differentiation and population structure currently defined for subdivided populations. $F_{ST}$ in practice typically assumes the "island model", where subpopulations have evolved independently from their last common ancestral population. In this work, we generalize the $F_{ST}$ definition to arbitrary population structures, where individuals may be related in arbitrary ways. Our definitions are built on identity-by-descent (IBD) probabilities that relate individuals through inbreeding and kinship coefficients. We generalize $F_{ST}$ as the mean inbreeding coefficient of the individuals' local populations relative to their last common ancestral population. This $F_{ST}$ naturally yields a useful pairwise $F_{ST}$ between individuals. We show that our generalized definition agrees with Wright's original and the island model definitions as special cases. We define a novel coancestry model based on "individual-specific allele frequencies" and prove that its parameters correspond to probabilistic kinship coefficients. Lastly, we study and extend the Pritchard-Stephens-Donnelly admixture model in the context of our coancestry model and calculate its $F_{ST}$. Our probabilistic framework provides a theoretical foundation that extends $F_{ST}$ in terms of inbreeding and kinship coefficients to arbitrary population structures.},
  date = {2016-10-27},
}

@Article{Ochoa083923,
  author = {Ochoa, Alejandro and Storey, John D.},
  title = {$F_{ST}$ and kinship for arbitrary population structures II: Method of moments estimators},
  year = {2016},
  doi = {10.1101/083923},
  publisher = {Cold Spring Harbor Labs Journals},
  url = {https://dx.doi.org/10.1101/083923},
  journal = {bioRxiv},
  volume = {doi:10.1101/083923},
  abstract = {$F_{ST}$ and kinship are key parameters often estimated in modern population genetics studies. Kinship matrices have also become a fundamental quantity used in genome-wide association studies and heritability estimation. The most frequently used estimators of FST and kinship are method of moments estimators whose accuracies depend strongly on the existence of simple underlying forms of structure, such as the island model of non-overlapping, independently evolving subpopulations. However, modern data sets have revealed that these simple models of structure do not likely hold in many populations, including humans. In this work, we provide new results on the behavior of these estimators in the presence of arbitrarily complex population structures. After establishing a framework for assessing bias and consistency of genome-wide estimators, we calculate the accuracy of $F_{ST}$ and kinship estimators under arbitrary population structures, characterizing biases and estimation challenges unobserved under their originally assumed models of structure. We illustrate our results using simulated genotypes from an admixture model, constructing a one-dimensional geographic scenario that departs nontrivially from the island model. Using 1000 Genomes Project data, we verify that population-level pairwise $F_{ST}$ estimates underestimate differentiation measured by an individual-level pairwise $F_{ST}$ estimator introduced here. We show that the calculated biases are due to unknown quantities that cannot be estimated under the established frameworks, highlighting the need for innovative estimation approaches in complex populations. We provide initial results that point towards a future estimation framework for generalized $F_{ST}$ and kinship.},
  date = {2016-10-27},
}

%% 2017

@Article{Hackett2017,
  author = {Hackett, Sean R. and Storey, John D.},
  title = {Mixed membership martial arts: Data-driven analysis of winning martial arts styles},
  journal = {MIT Sloan Sports Analytics Conference},
  year = {2017},
  url = {http://www.sloansportsconference.com/wp-content/uploads/2017/02/1575.pdf},  
  abstract = {A major analytics challenge in Mixed Martial Arts (MMA) is capturing the differences between fighters that are essential for both establishing matchups and facilitating fan understanding. Here, we model ~18,000 fighters as mixtures of 10 data-defined prototypical martial arts styles, each with characteristic ways of winning. By balancing fighter-level data with broader trends in MMA, fighter behavior can be predicted even for inexperienced fighters. Beyond providing an informative summary of a fighter's performance, style is a major determinant of success in MMA. This is reflected by the fact that champions of the sport conform to a narrow subset of successful styles.},
  date = {2017-03-03},
}

@Article{Chen241133,
  author = {Chen, Xiongzhi and Robinson, David G. and Storey, John D.},
  title = {The functional false discovery rate with applications to genomics},
  year = {2017},
  doi = {10.1101/241133},
  publisher = {Cold Spring Harbor Laboratory},
  url = {https://dx.doi.org/10.1101/241133},
  journal = {bioRxiv},
  volume = {doi:10.1101/241133},
  abstract = {The false discovery rate measures the proportion of false discoveries among a set of hypothesis tests called significant. This quantity is typically estimated based on p-values or test statistics. In some scenarios, there is additional information available that may be used to more accurately estimate the false discovery rate. We develop a new framework for formulating and estimating false discovery rates and q-values when an additional piece of information, which we call an 'informative variable', is available. For a given test, the informative variable provides information about the prior probability a null hypothesis is true or the power of that particular test. The false discovery rate is then treated as a function of this informative variable. We consider two applications in genomics. Our first is a genetics of gene expression (eQTL) experiment in yeast where every genetic marker and gene expression trait pair are tested for associations. The informative variable in this case is the distance between each genetic marker and gene. Our second application is to detect differentially expressed genes in an RNA-seq study carried out in mice. The informative variable in this study is the per-gene read depth. The framework we develop is quite general, and it should be useful in a broad range of scientific applications.},
  date = {2017-12-30},
}

@Article{Cabreros240812,
  author = {Cabreros, Irineo and Storey, John D.},
  title = {A nonparametric estimator of population structure unifying admixture models and principal components analysis},
  year = {2017},
  doi = {10.1101/240812},
  publisher = {Cold Spring Harbor Laboratory},
  url = {https://dx.doi.org/10.1101/240812},
  journal = {bioRxiv},
  volume = {doi:10.1101/240812},
  abstract = {We introduce a simple and computationally efficient method for fitting the admixture model of genetic population structure, called ALStructure. The strategy of ALStructure is to first estimate the low-dimensional linear subspace of the population admixture components and then search for a model within this subspace that is consistent with the admixture model's natural probabilistic constraints. Central to this strategy is the observation that all models belonging to this constrained space of solutions are risk-minimizing and have equal likelihood, rendering any additional optimization unnecessary. The low-dimensional linear subspace is estimated through a recently introduced principal components analysis method that is appropriate for genotype data, thereby providing a solution that has both principal components and probabilistic admixture interpretations. Our approach differs fundamentally from other existing methods for estimating admixture, which aim to fit the admixture model directly by searching for parameters that maximize the likelihood function or the posterior probability. We observe that ALStructure typically outperforms existing methods both in accuracy and computational speed under a wide array of simulated and real human genotype datasets. Throughout this work we emphasize that the admixture model is a special case of a much broader class of models for which algorithms similar to ALStructure may be successfully employed.},
  date = {2017-12-29},
}

@Article{Hao240804,
  author = {Hao, Wei and Storey, John D.},
  title = {Extending tests of Hardy-Weinberg equilibrium to structured populations},
  year = {2017},
  doi = {10.1101/240804},
  publisher = {Cold Spring Harbor Laboratory},
  url = {https://dx.doi.org/10.1101/240804},
  journal = {bioRxiv},
  volume = {doi:10.1101/240804},
  abstract = {Testing for Hardy-Weinberg equilibrium (HWE) is an important component in almost all analyses of population genetic data. Genetic markers that violate HWE are often treated as special cases; for example, they may be flagged as possible genotyping errors or they may be investigated more closely for evolutionary signatures of interest. The presence of population structure is one reason why genetic markers may fail a test of HWE. This is problematic because almost all natural populations studied in the modern setting show some degree of structure. Therefore, it is important to be able to detect deviations from HWE for reasons other than structure. To this end, we extend statistical tests of HWE to allow for population structure, which we call a test of 'structural HWE' (sHWE). Additionally, our new test allows one to automatically choose tuning parameters and identify accurate models of structure. We demonstrate our approach on several important studies, provide theoretical justification for the test, and present empirical evidence for its utility. We anticipate the proposed test will be useful in a broad range of analyses of genome-wide population genetic data.},
  date = {2017-12-29},
}

